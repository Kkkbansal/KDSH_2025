{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4deVEL_Dsnz"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "<h1 style=\"color: blue; font-weight: bold; text-align: center; font-size: 24px;\">Research Paper Publishability Assessment and  Conference Selection</h1>\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, HTML\n",
        "\n",
        "display(HTML('''\n",
        "<h1 style=\"color: blue; font-weight: bold; text-align: center; font-size: 48px;\">\n",
        "    Research Paper Publishability Assessment and Conference Selection\n",
        "</h1>\n",
        "'''))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 172
        },
        "id": "quA8HN-MH_pT",
        "outputId": "442fffcf-7a89-4c78-9b01-b6d5afbd6d90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<h1 style=\"color: blue; font-weight: bold; text-align: center; font-size: 48px;\">\n",
              "    Research Paper Publishability Assessment and Conference Selection\n",
              "</h1>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Full notebook"
      ],
      "metadata": {
        "id": "zsWk8gdxndYj"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyVASmxq7BaZ"
      },
      "source": [
        "<h1 style=\"color: blue; font-weight: bold; text-align: center;\">Importing necessary Libraries</h1>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ss4yLB7f-JbL"
      },
      "source": [
        "##we have installed and imported  necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U pathway[all]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YxRpjyDfWzv3",
        "outputId": "50a924ca-1ddb-49f5-8a8a-a56b6f9a4ab4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pathway[all]\n",
            "  Downloading pathway-0.16.4-cp310-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/40.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp>=3.8.4 in /usr/local/lib/python3.10/dist-packages (from pathway[all]) (3.11.11)\n",
            "Requirement already satisfied: click>=8.1 in /usr/local/lib/python3.10/dist-packages (from pathway[all]) (8.1.8)\n",
            "Requirement already satisfied: geopy>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from pathway[all]) (2.4.1)\n",
            "Collecting h3>=4 (from pathway[all])\n",
            "  Downloading h3-4.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.10/dist-packages (from pathway[all]) (1.26.4)\n",
            "Requirement already satisfied: pandas>=2.1 in /usr/local/lib/python3.10/dist-packages (from pathway[all]) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn>=1.0 in /usr/local/lib/python3.10/dist-packages (from pathway[all]) (1.6.0)\n",
            "Requirement already satisfied: shapely>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from pathway[all]) (2.0.6)\n",
            "Collecting sqlglot==10.6.1 (from pathway[all])\n",
            "  Downloading sqlglot-10.6.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: pyarrow>=10.0.0 in /usr/local/lib/python3.10/dist-packages (from pathway[all]) (17.0.0)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from pathway[all]) (2.32.3)\n",
            "Collecting python-sat>=0.1.8.dev0 (from pathway[all])\n",
            "  Downloading python_sat-1.8.dev14-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting beartype<0.16.0,>=0.14.0 (from pathway[all])\n",
            "  Downloading beartype-0.15.0-py3-none-any.whl.metadata (28 kB)\n",
            "Requirement already satisfied: rich>=12.6.0 in /usr/local/lib/python3.10/dist-packages (from pathway[all]) (13.9.4)\n",
            "Collecting diskcache>=5.2.1 (from pathway[all])\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: exceptiongroup>=1.1.3 in /usr/local/lib/python3.10/dist-packages (from pathway[all]) (1.2.2)\n",
            "Collecting boto3>=1.26.76 (from pathway[all])\n",
            "  Downloading boto3-1.35.98-py3-none-any.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: google-api-python-client>=2.108.0 in /usr/local/lib/python3.10/dist-packages (from pathway[all]) (2.155.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from pathway[all]) (4.12.2)\n",
            "Requirement already satisfied: panel>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from pathway[all]) (1.5.5)\n",
            "Collecting jupyter-bokeh>=3.0.7 (from pathway[all])\n",
            "  Downloading jupyter_bokeh-4.0.5-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting jmespath>=1.0.1 (from pathway[all])\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting aiohttp-cors>=0.7.0 (from pathway[all])\n",
            "  Downloading aiohttp_cors-0.7.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: opentelemetry-api>=1.22.0 in /usr/local/lib/python3.10/dist-packages (from pathway[all]) (1.29.0)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.22.0 in /usr/local/lib/python3.10/dist-packages (from pathway[all]) (1.29.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.22.0 (from pathway[all])\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.29.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting fs>=2.4.16 (from pathway[all])\n",
            "  Downloading fs-2.4.16-py2.py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting async-lru>=2.0.4 (from pathway[all])\n",
            "  Downloading async_lru-2.0.4-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: networkx>=3.2.1 in /usr/local/lib/python3.10/dist-packages (from pathway[all]) (3.4.2)\n",
            "Requirement already satisfied: google-cloud-pubsub>=2.21.1 in /usr/local/lib/python3.10/dist-packages (from pathway[all]) (2.27.2)\n",
            "Requirement already satisfied: google-cloud-bigquery in /usr/local/lib/python3.10/dist-packages (from pathway[all]) (3.25.0)\n",
            "Collecting pydantic~=2.9.0 (from pathway[all])\n",
            "  Downloading pydantic-2.9.2-py3-none-any.whl.metadata (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.4/149.4 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gitpython>=3.1.43 in /usr/local/lib/python3.10/dist-packages (from pathway[all]) (3.1.44)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.8.4->pathway[all]) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.8.4->pathway[all]) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.8.4->pathway[all]) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.8.4->pathway[all]) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.8.4->pathway[all]) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.8.4->pathway[all]) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.8.4->pathway[all]) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.8.4->pathway[all]) (1.18.3)\n",
            "Collecting botocore<1.36.0,>=1.35.98 (from boto3>=1.26.76->pathway[all])\n",
            "  Downloading botocore-1.35.98-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3>=1.26.76->pathway[all])\n",
            "  Downloading s3transfer-0.10.4-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting appdirs~=1.4.3 (from fs>=2.4.16->pathway[all])\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from fs>=2.4.16->pathway[all]) (75.1.0)\n",
            "Requirement already satisfied: six~=1.10 in /usr/local/lib/python3.10/dist-packages (from fs>=2.4.16->pathway[all]) (1.17.0)\n",
            "Requirement already satisfied: geographiclib<3,>=1.52 in /usr/local/lib/python3.10/dist-packages (from geopy>=2.4.0->pathway[all]) (2.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython>=3.1.43->pathway[all]) (4.0.12)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=2.108.0->pathway[all]) (0.22.0)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=2.108.0->pathway[all]) (2.27.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=2.108.0->pathway[all]) (0.2.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=2.108.0->pathway[all]) (2.19.2)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=2.108.0->pathway[all]) (4.1.1)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.51.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-pubsub>=2.21.1->pathway[all]) (1.69.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-pubsub>=2.21.1->pathway[all]) (1.25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-pubsub>=2.21.1->pathway[all]) (4.25.5)\n",
            "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /usr/local/lib/python3.10/dist-packages (from google-cloud-pubsub>=2.21.1->pathway[all]) (0.14.0)\n",
            "Requirement already satisfied: grpcio-status>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-pubsub>=2.21.1->pathway[all]) (1.62.3)\n",
            "Requirement already satisfied: bokeh==3.* in /usr/local/lib/python3.10/dist-packages (from jupyter-bokeh>=3.0.7->pathway[all]) (3.6.2)\n",
            "Collecting ipywidgets==8.* (from jupyter-bokeh>=3.0.7->pathway[all])\n",
            "  Downloading ipywidgets-8.1.5-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: Jinja2>=2.9 in /usr/local/lib/python3.10/dist-packages (from bokeh==3.*->jupyter-bokeh>=3.0.7->pathway[all]) (3.1.5)\n",
            "Requirement already satisfied: contourpy>=1.2 in /usr/local/lib/python3.10/dist-packages (from bokeh==3.*->jupyter-bokeh>=3.0.7->pathway[all]) (1.3.1)\n",
            "Requirement already satisfied: packaging>=16.8 in /usr/local/lib/python3.10/dist-packages (from bokeh==3.*->jupyter-bokeh>=3.0.7->pathway[all]) (24.2)\n",
            "Requirement already satisfied: pillow>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from bokeh==3.*->jupyter-bokeh>=3.0.7->pathway[all]) (11.1.0)\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.10/dist-packages (from bokeh==3.*->jupyter-bokeh>=3.0.7->pathway[all]) (6.0.2)\n",
            "Requirement already satisfied: tornado>=6.2 in /usr/local/lib/python3.10/dist-packages (from bokeh==3.*->jupyter-bokeh>=3.0.7->pathway[all]) (6.3.3)\n",
            "Requirement already satisfied: xyzservices>=2021.09.1 in /usr/local/lib/python3.10/dist-packages (from bokeh==3.*->jupyter-bokeh>=3.0.7->pathway[all]) (2024.9.0)\n",
            "Collecting comm>=0.1.3 (from ipywidgets==8.*->jupyter-bokeh>=3.0.7->pathway[all])\n",
            "  Downloading comm-0.2.2-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets==8.*->jupyter-bokeh>=3.0.7->pathway[all]) (7.34.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets==8.*->jupyter-bokeh>=3.0.7->pathway[all]) (5.7.1)\n",
            "Collecting widgetsnbextension~=4.0.12 (from ipywidgets==8.*->jupyter-bokeh>=3.0.7->pathway[all])\n",
            "  Downloading widgetsnbextension-4.0.13-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in /usr/local/lib/python3.10/dist-packages (from ipywidgets==8.*->jupyter-bokeh>=3.0.7->pathway[all]) (3.0.13)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.22.0->pathway[all]) (1.2.15)\n",
            "Requirement already satisfied: importlib-metadata<=8.5.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.22.0->pathway[all]) (8.5.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.22.0->pathway[all]) (1.66.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.29.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.22.0->pathway[all])\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.29.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.29.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.22.0->pathway[all])\n",
            "  Downloading opentelemetry_proto-1.29.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2 (from google-cloud-pubsub>=2.21.1->pathway[all])\n",
            "  Downloading protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.50b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-sdk>=1.22.0->pathway[all]) (0.50b0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.1->pathway[all]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.1->pathway[all]) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.1->pathway[all]) (2024.2)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from panel>=1.3.1->pathway[all]) (6.2.0)\n",
            "Requirement already satisfied: linkify-it-py in /usr/local/lib/python3.10/dist-packages (from panel>=1.3.1->pathway[all]) (2.0.3)\n",
            "Requirement already satisfied: markdown in /usr/local/lib/python3.10/dist-packages (from panel>=1.3.1->pathway[all]) (3.7)\n",
            "Requirement already satisfied: markdown-it-py in /usr/local/lib/python3.10/dist-packages (from panel>=1.3.1->pathway[all]) (3.0.0)\n",
            "Requirement already satisfied: mdit-py-plugins in /usr/local/lib/python3.10/dist-packages (from panel>=1.3.1->pathway[all]) (0.4.2)\n",
            "Requirement already satisfied: param<3.0,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from panel>=1.3.1->pathway[all]) (2.2.0)\n",
            "Requirement already satisfied: pyviz-comms>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from panel>=1.3.1->pathway[all]) (3.0.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from panel>=1.3.1->pathway[all]) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic~=2.9.0->pathway[all]) (0.7.0)\n",
            "Collecting pydantic-core==2.23.4 (from pydantic~=2.9.0->pathway[all])\n",
            "  Downloading pydantic_core-2.23.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->pathway[all]) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->pathway[all]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->pathway[all]) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->pathway[all]) (2024.12.14)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.6.0->pathway[all]) (2.18.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0->pathway[all]) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0->pathway[all]) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0->pathway[all]) (3.5.0)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery->pathway[all]) (2.4.1)\n",
            "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery->pathway[all]) (2.7.2)\n",
            "Collecting google-cloud-run (from pathway[all])\n",
            "  Downloading google_cloud_run-0.10.14-py2.py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting google-cloud-secret-manager (from pathway[all])\n",
            "  Downloading google_cloud_secret_manager-2.22.0-py2.py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting google-cloud-logging (from pathway[all])\n",
            "  Downloading google_cloud_logging-3.11.3-py2.py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting python-docx>=1.1.2 (from pathway[all])\n",
            "  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting openparse==0.5.6 (from pathway[all])\n",
            "  Downloading openparse-0.5.6-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting unstructured>=0.16 (from pathway[all])\n",
            "  Downloading unstructured-0.16.13-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting pdf2image (from pathway[all])\n",
            "  Downloading pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting pypdf (from pathway[all])\n",
            "  Downloading pypdf-5.1.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting PyMuPDF>=1.23.2 (from openparse==0.5.6->pathway[all])\n",
            "  Downloading pymupdf-1.25.1-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting pdfminer.six>=20200401 (from openparse==0.5.6->pathway[all])\n",
            "  Downloading pdfminer.six-20240706-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting tiktoken>=0.3 (from openparse==0.5.6->pathway[all])\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: openai>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from openparse==0.5.6->pathway[all]) (1.59.4)\n",
            "Collecting surya-ocr==0.4.14 (from pathway[all])\n",
            "  Downloading surya_ocr-0.4.14-py3-none-any.whl.metadata (26 kB)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (from pathway[all]) (3.3.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from pathway[all]) (4.47.1)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from surya-ocr==0.4.14->pathway[all])\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting ftfy<7.0.0,>=6.1.3 (from surya-ocr==0.4.14->pathway[all])\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: opencv-python<5.0.0.0,>=4.9.0.80 in /usr/local/lib/python3.10/dist-packages (from surya-ocr==0.4.14->pathway[all]) (4.10.0.84)\n",
            "Collecting pillow>=7.1.0 (from bokeh==3.*->jupyter-bokeh>=3.0.7->pathway[all])\n",
            "  Downloading pillow-10.4.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.1.0 (from surya-ocr==0.4.14->pathway[all])\n",
            "  Downloading pydantic_settings-2.7.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting pypdfium2<5.0.0,>=4.25.0 (from surya-ocr==0.4.14->pathway[all])\n",
            "  Downloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv<2.0.0,>=1.0.0 (from surya-ocr==0.4.14->pathway[all])\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: tabulate<0.10.0,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from surya-ocr==0.4.14->pathway[all]) (0.9.0)\n",
            "Requirement already satisfied: torch<3.0.0,>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from surya-ocr==0.4.14->pathway[all]) (2.5.1+cu121)\n",
            "Collecting litellm~=1.44.8 (from pathway[all])\n",
            "  Downloading litellm-1.44.28-py3-none-any.whl.metadata (32 kB)\n",
            "Collecting cohere~=5.1.0 (from pathway[all])\n",
            "  Downloading cohere-5.1.8-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting langchain-core==0.1.30 (from pathway[all])\n",
            "  Downloading langchain_core-0.1.30-py3-none-any.whl.metadata (6.0 kB)\n",
            "Collecting langchain==0.1.11 (from pathway[all])\n",
            "  Downloading langchain-0.1.11-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting llama-index-core~=0.10.0 (from pathway[all])\n",
            "  Downloading llama_index_core-0.10.68.post1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting llama-index-readers-pathway~=0.1.0 (from pathway[all])\n",
            "  Downloading llama_index_readers_pathway-0.1.3-py3-none-any.whl.metadata (639 bytes)\n",
            "Collecting llama-index-retrievers-pathway~=0.1.0 (from pathway[all])\n",
            "  Downloading llama_index_retrievers_pathway-0.1.3-py3-none-any.whl.metadata (658 bytes)\n",
            "Requirement already satisfied: tenacity!=8.4.0 in /usr/local/lib/python3.10/dist-packages (from pathway[all]) (9.0.0)\n",
            "Collecting instructor==1.2.6 (from pathway[all])\n",
            "  Downloading instructor-1.2.6-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: google-generativeai>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from pathway[all]) (0.8.3)\n",
            "Requirement already satisfied: docstring-parser<0.17,>=0.16 in /usr/local/lib/python3.10/dist-packages (from instructor==1.2.6->pathway[all]) (0.16)\n",
            "Collecting tenacity!=8.4.0 (from pathway[all])\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from instructor==1.2.6->pathway[all]) (0.15.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.11->pathway[all]) (2.0.36)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain==0.1.11->pathway[all])\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.11->pathway[all]) (1.33)\n",
            "Collecting langchain-community<0.1,>=0.0.25 (from langchain==0.1.11->pathway[all])\n",
            "  Downloading langchain_community-0.0.38-py3-none-any.whl.metadata (8.7 kB)\n",
            "Collecting langchain-text-splitters<0.1,>=0.0.1 (from langchain==0.1.11->pathway[all])\n",
            "  Downloading langchain_text_splitters-0.0.2-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain==0.1.11->pathway[all])\n",
            "  Downloading langsmith-0.1.147-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core==0.1.30->pathway[all]) (3.7.1)\n",
            "Collecting packaging>=16.8 (from bokeh==3.*->jupyter-bokeh>=3.0.7->pathway[all])\n",
            "  Downloading packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting office365-rest-python-client>=2.5.3 (from pathway[all])\n",
            "  Downloading Office365_REST_Python_Client-2.5.14-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting fastavro<2.0.0,>=1.9.4 (from cohere~=5.1.0->pathway[all])\n",
            "  Downloading fastavro-1.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: httpx>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from cohere~=5.1.0->pathway[all]) (0.28.1)\n",
            "Collecting types-requests<3.0.0.0,>=2.31.0.20240311 (from cohere~=5.1.0->pathway[all])\n",
            "  Downloading types_requests-2.32.0.20241016-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.6->opentelemetry-api>=1.22.0->pathway[all]) (1.17.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython>=3.1.43->pathway[all]) (5.0.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client>=2.108.0->pathway[all]) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client>=2.108.0->pathway[all]) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client>=2.108.0->pathway[all]) (4.9)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.10 in /usr/local/lib/python3.10/dist-packages (from google-generativeai>=0.7.0->pathway[all]) (0.6.10)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery->pathway[all]) (1.6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client>=2.108.0->pathway[all]) (3.2.1)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api>=1.22.0->pathway[all]) (3.21.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=2.9->bokeh==3.*->jupyter-bokeh>=3.0.7->pathway[all]) (3.0.2)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from litellm~=1.44.8->pathway[all]) (4.23.0)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (from litellm~=1.44.8->pathway[all]) (0.21.0)\n",
            "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core~=0.10.0->pathway[all])\n",
            "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core~=0.10.0->pathway[all]) (2024.10.0)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core~=0.10.0->pathway[all]) (1.6.0)\n",
            "Requirement already satisfied: nltk!=3.9,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core~=0.10.0->pathway[all]) (3.9.1)\n",
            "Collecting typing-inspect>=0.8.0 (from llama-index-core~=0.10.0->pathway[all])\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py->panel>=1.3.1->pathway[all]) (0.1.2)\n",
            "Collecting msal (from office365-rest-python-client>=2.5.3->pathway[all])\n",
            "  Downloading msal-1.31.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.0.0->openparse==0.5.6->pathway[all]) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.0.0->openparse==0.5.6->pathway[all]) (0.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>=1.0.0->openparse==0.5.6->pathway[all]) (1.3.1)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-docx>=1.1.2->pathway[all]) (5.3.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken>=0.3->openparse==0.5.6->pathway[all]) (2024.11.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers->pathway[all]) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers->pathway[all]) (0.27.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers->pathway[all]) (0.5.1)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from unstructured>=0.16->pathway[all]) (5.2.0)\n",
            "Collecting python-magic (from unstructured>=0.16->pathway[all])\n",
            "  Downloading python_magic-0.4.27-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from unstructured>=0.16->pathway[all]) (4.12.3)\n",
            "Collecting emoji (from unstructured>=0.16->pathway[all])\n",
            "  Downloading emoji-2.14.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting python-iso639 (from unstructured>=0.16->pathway[all])\n",
            "  Downloading python_iso639-2024.10.22-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting langdetect (from unstructured>=0.16->pathway[all])\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting rapidfuzz (from unstructured>=0.16->pathway[all])\n",
            "  Downloading rapidfuzz-3.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting backoff (from unstructured>=0.16->pathway[all])\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting unstructured-client (from unstructured>=0.16->pathway[all])\n",
            "  Downloading unstructured_client-0.28.1-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from unstructured>=0.16->pathway[all]) (5.9.5)\n",
            "Collecting python-oxmsg (from unstructured>=0.16->pathway[all])\n",
            "  Downloading python_oxmsg-0.0.1-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: html5lib in /usr/local/lib/python3.10/dist-packages (from unstructured>=0.16->pathway[all]) (1.1)\n",
            "Collecting ndjson (from unstructured>=0.16->pathway[all])\n",
            "  Downloading ndjson-0.3.1-py2.py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]>=0.16; extra == \"xpack-llm-local\"->pathway[all]) (3.1.5)\n",
            "Collecting onnx (from unstructured[all-docs]>=0.16; extra == \"xpack-llm-local\"->pathway[all])\n",
            "  Downloading onnx-1.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Collecting unstructured-inference==0.8.1 (from unstructured[all-docs]>=0.16; extra == \"xpack-llm-local\"->pathway[all])\n",
            "  Downloading unstructured_inference-0.8.1-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting python-pptx>=1.0.1 (from unstructured[all-docs]>=0.16; extra == \"xpack-llm-local\"->pathway[all])\n",
            "  Downloading python_pptx-1.0.2-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting pypandoc (from unstructured[all-docs]>=0.16; extra == \"xpack-llm-local\"->pathway[all])\n",
            "  Downloading pypandoc-1.15-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting unstructured.pytesseract>=0.3.12 (from unstructured[all-docs]>=0.16; extra == \"xpack-llm-local\"->pathway[all])\n",
            "  Downloading unstructured.pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting pikepdf (from unstructured[all-docs]>=0.16; extra == \"xpack-llm-local\"->pathway[all])\n",
            "  Downloading pikepdf-9.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting pi-heif (from unstructured[all-docs]>=0.16; extra == \"xpack-llm-local\"->pathway[all])\n",
            "  Downloading pi_heif-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\n",
            "Collecting effdet (from unstructured[all-docs]>=0.16; extra == \"xpack-llm-local\"->pathway[all])\n",
            "  Downloading effdet-0.4.1-py3-none-any.whl.metadata (33 kB)\n",
            "Collecting google-cloud-vision (from unstructured[all-docs]>=0.16; extra == \"xpack-llm-local\"->pathway[all])\n",
            "  Downloading google_cloud_vision-3.9.0-py2.py3-none-any.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: xlrd in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]>=0.16; extra == \"xpack-llm-local\"->pathway[all]) (2.0.1)\n",
            "Collecting layoutparser (from unstructured-inference==0.8.1->unstructured[all-docs]>=0.16; extra == \"xpack-llm-local\"->pathway[all])\n",
            "  Downloading layoutparser-0.3.4-py3-none-any.whl.metadata (7.7 kB)\n",
            "Collecting python-multipart (from unstructured-inference==0.8.1->unstructured[all-docs]>=0.16; extra == \"xpack-llm-local\"->pathway[all])\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting onnxruntime>=1.17.0 (from unstructured-inference==0.8.1->unstructured[all-docs]>=0.16; extra == \"xpack-llm-local\"->pathway[all])\n",
            "  Downloading onnxruntime-1.20.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.8.1->unstructured[all-docs]>=0.16; extra == \"xpack-llm-local\"->pathway[all]) (3.10.0)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.8.1->unstructured[all-docs]>=0.16; extra == \"xpack-llm-local\"->pathway[all]) (1.0.12)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->panel>=1.3.1->pathway[all]) (0.5.1)\n",
            "Collecting google-cloud-appengine-logging<2.0.0dev,>=0.1.3 (from google-cloud-logging->pathway[all])\n",
            "  Downloading google_cloud_appengine_logging-1.5.0-py2.py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting google-cloud-audit-log<1.0.0dev,>=0.2.4 (from google-cloud-logging->pathway[all])\n",
            "  Downloading google_cloud_audit_log-0.3.0-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: uc-micro-py in /usr/local/lib/python3.10/dist-packages (from linkify-it-py->panel>=1.3.1->pathway[all]) (1.0.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.11->pathway[all])\n",
            "  Downloading marshmallow-3.25.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy<7.0.0,>=6.1.3->surya-ocr==0.4.14->pathway[all]) (0.2.13)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere~=5.1.0->pathway[all]) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.21.2->cohere~=5.1.0->pathway[all]) (0.14.0)\n",
            "Collecting jedi>=0.16 (from ipython>=6.1.0->ipywidgets==8.*->jupyter-bokeh>=3.0.7->pathway[all])\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets==8.*->jupyter-bokeh>=3.0.7->pathway[all]) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets==8.*->jupyter-bokeh>=3.0.7->pathway[all]) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets==8.*->jupyter-bokeh>=3.0.7->pathway[all]) (3.0.48)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets==8.*->jupyter-bokeh>=3.0.7->pathway[all]) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets==8.*->jupyter-bokeh>=3.0.7->pathway[all]) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets==8.*->jupyter-bokeh>=3.0.7->pathway[all]) (4.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain==0.1.11->pathway[all]) (3.0.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm~=1.44.8->pathway[all]) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm~=1.44.8->pathway[all]) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm~=1.44.8->pathway[all]) (0.22.3)\n",
            "INFO: pip is looking at multiple versions of langchain-community to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting langchain-community<0.1,>=0.0.25 (from langchain==0.1.11->pathway[all])\n",
            "  Downloading langchain_community-0.0.37-py3-none-any.whl.metadata (8.7 kB)\n",
            "  Downloading langchain_community-0.0.36-py3-none-any.whl.metadata (8.7 kB)\n",
            "  Downloading langchain_community-0.0.35-py3-none-any.whl.metadata (8.7 kB)\n",
            "  Downloading langchain_community-0.0.34-py3-none-any.whl.metadata (8.5 kB)\n",
            "  Downloading langchain_community-0.0.33-py3-none-any.whl.metadata (8.5 kB)\n",
            "  Downloading langchain_community-0.0.32-py3-none-any.whl.metadata (8.5 kB)\n",
            "  Downloading langchain_community-0.0.31-py3-none-any.whl.metadata (8.4 kB)\n",
            "INFO: pip is still looking at multiple versions of langchain-community to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading langchain_community-0.0.30-py3-none-any.whl.metadata (8.4 kB)\n",
            "  Downloading langchain_community-0.0.29-py3-none-any.whl.metadata (8.3 kB)\n",
            "  Downloading langchain_community-0.0.28-py3-none-any.whl.metadata (8.3 kB)\n",
            "  Downloading langchain_community-0.0.27-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.1.11->pathway[all]) (3.10.13)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.1.11->pathway[all]) (1.0.0)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six>=20200401->openparse==0.5.6->pathway[all]) (43.0.3)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client>=2.108.0->pathway[all]) (0.6.1)\n",
            "Collecting XlsxWriter>=0.5.7 (from python-pptx>=1.0.1->unstructured[all-docs]>=0.16; extra == \"xpack-llm-local\"->pathway[all])\n",
            "  Downloading XlsxWriter-3.2.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.1.11->pathway[all]) (3.1.1)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.3.0->surya-ocr==0.4.14->pathway[all]) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch<3.0.0,>=2.3.0->surya-ocr==0.4.14->pathway[all]) (1.3.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.9.0->instructor==1.2.6->pathway[all]) (1.5.4)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core~=0.10.0->pathway[all])\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->unstructured>=0.16->pathway[all]) (2.6)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from effdet->unstructured[all-docs]>=0.16; extra == \"xpack-llm-local\"->pathway[all]) (0.20.1+cu121)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from effdet->unstructured[all-docs]>=0.16; extra == \"xpack-llm-local\"->pathway[all]) (2.0.8)\n",
            "Collecting omegaconf>=2.0 (from effdet->unstructured[all-docs]>=0.16; extra == \"xpack-llm-local\"->pathway[all])\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: PyJWT<3,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from PyJWT[crypto]<3,>=1.0.0->msal->office365-rest-python-client>=2.5.3->pathway[all]) (2.10.1)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl->unstructured[all-docs]>=0.16; extra == \"xpack-llm-local\"->pathway[all]) (2.0.0)\n",
            "Collecting olefile (from python-oxmsg->unstructured>=0.16->pathway[all])\n",
            "  Downloading olefile-0.47-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting aiofiles>=24.1.0 (from unstructured-client->unstructured>=0.16->pathway[all])\n",
            "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: eval-type-backport<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured>=0.16->pathway[all]) (0.2.2)\n",
            "Collecting jsonpath-python<2.0.0,>=1.0.6 (from unstructured-client->unstructured>=0.16->pathway[all])\n",
            "  Downloading jsonpath_python-1.0.6-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six>=20200401->openparse==0.5.6->pathway[all]) (1.17.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets==8.*->jupyter-bokeh>=3.0.7->pathway[all]) (0.8.4)\n",
            "Collecting antlr4-python3-runtime==4.9.* (from omegaconf>=2.0->effdet->unstructured[all-docs]>=0.16; extra == \"xpack-llm-local\"->pathway[all])\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting coloredlogs (from onnxruntime>=1.17.0->unstructured-inference==0.8.1->unstructured[all-docs]>=0.16; extra == \"xpack-llm-local\"->pathway[all])\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.17.0->unstructured-inference==0.8.1->unstructured[all-docs]>=0.16; extra == \"xpack-llm-local\"->pathway[all]) (24.12.23)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets==8.*->jupyter-bokeh>=3.0.7->pathway[all]) (0.7.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->unstructured-inference==0.8.1->unstructured[all-docs]>=0.16; extra == \"xpack-llm-local\"->pathway[all]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->unstructured-inference==0.8.1->unstructured[all-docs]>=0.16; extra == \"xpack-llm-local\"->pathway[all]) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->unstructured-inference==0.8.1->unstructured[all-docs]>=0.16; extra == \"xpack-llm-local\"->pathway[all]) (1.4.8)\n",
            "Collecting iopath (from layoutparser->unstructured-inference==0.8.1->unstructured[all-docs]>=0.16; extra == \"xpack-llm-local\"->pathway[all])\n",
            "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pdfplumber (from layoutparser->unstructured-inference==0.8.1->unstructured[all-docs]>=0.16; extra == \"xpack-llm-local\"->pathway[all])\n",
            "  Downloading pdfplumber-0.11.5-py3-none-any.whl.metadata (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.5/42.5 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six>=20200401->openparse==0.5.6->pathway[all]) (2.22)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.17.0->unstructured-inference==0.8.1->unstructured[all-docs]>=0.16; extra == \"xpack-llm-local\"->pathway[all])\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting portalocker (from iopath->layoutparser->unstructured-inference==0.8.1->unstructured[all-docs]>=0.16; extra == \"xpack-llm-local\"->pathway[all])\n",
            "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting pdfminer.six>=20200401 (from openparse==0.5.6->pathway[all])\n",
            "  Downloading pdfminer.six-20231228-py3-none-any.whl.metadata (4.2 kB)\n",
            "Downloading sqlglot-10.6.1-py3-none-any.whl (223 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.4/223.4 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiohttp_cors-0.7.0-py3-none-any.whl (27 kB)\n",
            "Downloading async_lru-2.0.4-py3-none-any.whl (6.1 kB)\n",
            "Downloading beartype-0.15.0-py3-none-any.whl (777 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m777.6/777.6 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading boto3-1.35.98-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.2/139.2 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fs-2.4.16-py2.py3-none-any.whl (135 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.3/135.3 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h3-4.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (993 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m993.5/993.5 kB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading jupyter_bokeh-4.0.5-py3-none-any.whl (148 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m148.6/148.6 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ipywidgets-8.1.5-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.29.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.29.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.29.0-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic-2.9.2-py3-none-any.whl (434 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.9/434.9 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_core-2.23.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_sat-1.8.dev14-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_28_x86_64.whl (2.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pathway-0.16.4-cp310-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (61.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openparse-0.5.6-py3-none-any.whl (89 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.5/89.5 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading surya_ocr-0.4.14-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.5/94.5 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading instructor-1.2.6-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain-0.1.11-py3-none-any.whl (807 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m807.5/807.5 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.1.30-py3-none-any.whl (256 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m256.9/256.9 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading botocore-1.35.98-py3-none-any.whl (13.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cohere-5.1.8-py3-none-any.whl (145 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.3/145.3 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading litellm-1.44.28-py3-none-any.whl (8.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_core-0.10.68.post1-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_readers_pathway-0.1.3-py3-none-any.whl (3.1 kB)\n",
            "Downloading llama_index_retrievers_pathway-0.1.3-py3-none-any.whl (3.3 kB)\n",
            "Downloading Office365_REST_Python_Client-2.5.14-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf-5.1.0-py3-none-any.whl (297 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading s3transfer-0.10.4-py3-none-any.whl (83 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.2/83.2 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading unstructured-0.16.13-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading unstructured_inference-0.8.1-py3-none-any.whl (48 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.4/48.4 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_logging-3.11.3-py2.py3-none-any.whl (218 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m218.9/218.9 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_run-0.10.14-py2.py3-none-any.whl (323 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.3/323.3 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_secret_manager-2.22.0-py2.py3-none-any.whl (208 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.2/208.2 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
            "Downloading comm-0.2.2-py3-none-any.whl (7.2 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
            "Downloading fastavro-1.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_appengine_logging-1.5.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading google_cloud_audit_log-0.3.0-py2.py3-none-any.whl (27 kB)\n",
            "Downloading langchain_community-0.0.27-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-0.0.2-py3-none-any.whl (23 kB)\n",
            "Downloading langsmith-0.1.147-py3-none-any.whl (311 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.8/311.8 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow-10.4.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.7.1-py3-none-any.whl (29 kB)\n",
            "Downloading pymupdf-1.25.1-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (20.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading python_pptx-1.0.2-py3-none-any.whl (472 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.8/472.8 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading types_requests-2.32.0.20241016-py3-none-any.whl (15 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading unstructured.pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
            "Downloading widgetsnbextension-4.0.13-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading effdet-0.4.1-py3-none-any.whl (112 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.5/112.5 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading emoji-2.14.0-py3-none-any.whl (586 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m586.9/586.9 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_vision-3.9.0-py2.py3-none-any.whl (514 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m514.6/514.6 kB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading msal-1.31.1-py3-none-any.whl (113 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.2/113.2 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ndjson-0.3.1-py2.py3-none-any.whl (5.3 kB)\n",
            "Downloading onnx-1.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pi_heif-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pikepdf-9.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypandoc-1.15-py3-none-any.whl (21 kB)\n",
            "Downloading python_iso639-2024.10.22-py3-none-any.whl (274 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.9/274.9 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
            "Downloading python_oxmsg-0.0.1-py3-none-any.whl (31 kB)\n",
            "Downloading rapidfuzz-3.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m83.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading unstructured_client-0.28.1-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.9/62.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpath_python-1.0.6-py3-none-any.whl (7.6 kB)\n",
            "Downloading marshmallow-3.25.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.20.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m80.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading XlsxWriter-3.2.0-py3-none-any.whl (159 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.9/159.9 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading layoutparser-0.3.4-py3-none-any.whl (19.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m71.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading olefile-0.47-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfplumber-0.11.5-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m84.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
            "Building wheels for collected packages: langdetect, antlr4-python3-runtime, iopath\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993222 sha256=59f480d963f87c598217a2e34740e231b1e3ef80530ac5121b8aae58450ff2d5\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144555 sha256=e5e617798bf8d36f3d27ebb111f3ea4af1c9ca27952fedacd8ec6e79146d9bdb\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31528 sha256=2c6610ab47e525d78663ddc0f32b440834049fc3708da4844a600a523fa92723\n",
            "  Stored in directory: /root/.cache/pip/wheels/9a/a3/b6/ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d\n",
            "Successfully built langdetect antlr4-python3-runtime iopath\n",
            "Installing collected packages: sqlglot, ndjson, filetype, dirtyjson, appdirs, antlr4-python3-runtime, XlsxWriter, widgetsnbextension, types-requests, tenacity, rapidfuzz, python-sat, python-multipart, python-magic, python-iso639, python-dotenv, python-docx, pypdfium2, pypdf, pypandoc, PyMuPDF, pydantic-core, protobuf, portalocker, pillow, packaging, omegaconf, olefile, mypy-extensions, langdetect, jsonpath-python, jmespath, jedi, humanfriendly, h3, ftfy, fs, fastavro, emoji, diskcache, comm, beartype, backoff, async-lru, aiofiles, unstructured.pytesseract, typing-inspect, tiktoken, python-pptx, python-oxmsg, pydantic, pikepdf, pi-heif, pdf2image, opentelemetry-proto, onnx, marshmallow, iopath, coloredlogs, botocore, unstructured-client, s3transfer, pydantic-settings, pdfminer.six, opentelemetry-exporter-otlp-proto-common, onnxruntime, langsmith, ipywidgets, google-cloud-audit-log, dataclasses-json, cohere, unstructured, pdfplumber, openparse, msal, llama-index-core, litellm, langchain-core, jupyter-bokeh, instructor, boto3, aiohttp-cors, surya-ocr, opentelemetry-exporter-otlp-proto-grpc, office365-rest-python-client, llama-index-retrievers-pathway, llama-index-readers-pathway, layoutparser, langchain-text-splitters, langchain-community, google-cloud-vision, google-cloud-secret-manager, google-cloud-run, google-cloud-appengine-logging, effdet, unstructured-inference, pathway, langchain, google-cloud-logging\n",
            "  Attempting uninstall: sqlglot\n",
            "    Found existing installation: sqlglot 25.1.0\n",
            "    Uninstalling sqlglot-25.1.0:\n",
            "      Successfully uninstalled sqlglot-25.1.0\n",
            "  Attempting uninstall: widgetsnbextension\n",
            "    Found existing installation: widgetsnbextension 3.6.10\n",
            "    Uninstalling widgetsnbextension-3.6.10:\n",
            "      Successfully uninstalled widgetsnbextension-3.6.10\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "  Attempting uninstall: pydantic-core\n",
            "    Found existing installation: pydantic_core 2.27.2\n",
            "    Uninstalling pydantic_core-2.27.2:\n",
            "      Successfully uninstalled pydantic_core-2.27.2\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 4.25.5\n",
            "    Uninstalling protobuf-4.25.5:\n",
            "      Successfully uninstalled protobuf-4.25.5\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: pillow 11.1.0\n",
            "    Uninstalling pillow-11.1.0:\n",
            "      Successfully uninstalled pillow-11.1.0\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.2\n",
            "    Uninstalling packaging-24.2:\n",
            "      Successfully uninstalled packaging-24.2\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.10.4\n",
            "    Uninstalling pydantic-2.10.4:\n",
            "      Successfully uninstalled pydantic-2.10.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DTtnJsBk_XO9"
      },
      "outputs": [],
      "source": [
        "!pip install pathway"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gw5cW0BF_maV"
      },
      "outputs": [],
      "source": [
        "!apt-get install poppler-utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AiIBQE7Q_gU1"
      },
      "outputs": [],
      "source": [
        "!pip install \"pathway[xpack-llm-docs]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yu4B7M2k_8oX"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade tiktoken -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7pzLvJft_vA8"
      },
      "outputs": [],
      "source": [
        "import pathway  as pw\n",
        "import openparse as op\n",
        "import tiktoken\n",
        "from pathway.xpacks.llm.parsers import ParseUnstructured\n",
        "from pathway.xpacks.llm.splitters import TokenCountSplitter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxDtKgzmA_oM"
      },
      "source": [
        "<h2 style=\"color: blue; font-weight: bold; text-align: center;\"><h1 style=\"color: blue; font-weight: bold; text-align: center; font-size: 24px;\">Dataset Processing </h2>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11wTLo0-AQIt"
      },
      "source": [
        " ### importing the dataset using google drive connector and parsing it to extract data in form of text and storing it form of  dataframe.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XSEccscTAljq"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I8K91mu0Ap25"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Define the file path\n",
        "file_path = '/content/drive/My Drive/KDSH/credentials.json'\n",
        "\n",
        "# Open and load the JSON file\n",
        "with open(file_path, 'r') as file:\n",
        "    data = json.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUkK5YL3B7Rp"
      },
      "outputs": [],
      "source": [
        "@pw.udf\n",
        "def strip_metadata(docs:list[tuple[str, dict]]) -> list[str]:\n",
        "    return [doc[0] for doc in docs]\n",
        "\n",
        "table = pw.io.gdrive.read(\n",
        "    object_id=\"1Z8z4craj36ighb8hzUzeM76OOgpUdsKr\",\n",
        "    service_user_credentials_file=file_path,\n",
        "    with_metadata=True,\n",
        "    mode=\"static\"\n",
        ")\n",
        "\n",
        "\n",
        "parser=  ParseUnstructured()\n",
        "doc= table.select(text=parser(pw.this.data))\n",
        "doc=doc.select(text=strip_metadata(pw.this.text))\n",
        "df=pw.debug.table_to_pandas(doc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "go4vqcYHKd9v"
      },
      "outputs": [],
      "source": [
        "df.to_csv('output.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CR-2j391KiRx"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df=pd.read_csv('output.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6tBYXNVAymK"
      },
      "source": [
        "<p style=\"color: black; font-weight: normal; text-align: left; font-size: 14px;\">.</p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcxvRMijJ8n4"
      },
      "source": [
        "<h2 style=\"color: blue; font-weight: bold; text-align: center;\"><h1 style=\"color: blue; font-weight: bold; text-align: center; font-size: 24px;\">Text Preprocessing </h2>\n",
        "\n",
        "###Dividing in sections and text preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "orlMyPSje8dZ"
      },
      "outputs": [],
      "source": [
        "for i in range(150):\n",
        "  df['text'][i]=df['text'][i][0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3L5unCJvXLQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4WQI1f6cJYQ9"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def structure_text(text):\n",
        "    \"\"\"\n",
        "    Structure the text into sections based on the presence of 'Abstract'\n",
        "    and the presence of a number followed by 1-2 words between line breaks.\n",
        "    :param text: Raw extracted text from the PDF.\n",
        "    :return: Dictionary containing structured sections.\n",
        "    \"\"\"\n",
        "    # Replace literal \\\\n with actual newlines\n",
        "    text = text.replace(\"\\\\n\", \"\\n\")\n",
        "\n",
        "    sections = {}\n",
        "    lines = text.splitlines()\n",
        "    current_section = \"Topic Name\"\n",
        "    sections[current_section] = \"\"\n",
        "\n",
        "    # Regex to detect a line with a number followed by 1-2 words (e.g., \"1 Introduction\")\n",
        "    section_pattern = re.compile(r\"^\\d+\\s+[A-Za-z]+(?:\\s+[A-Za-z]+)?$\")  # Allow 1 or 2 words after the number\n",
        "    abstract_found = False\n",
        "\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "\n",
        "        # Detect Abstract as the first section\n",
        "        if not abstract_found and \"abstract\" in line.lower():\n",
        "            current_section = \"Abstract\"\n",
        "            sections[current_section] = \"\"\n",
        "            abstract_found = True\n",
        "            continue\n",
        "\n",
        "        # Check if the line matches the section pattern (e.g., \"1 Introduction\")\n",
        "        if section_pattern.match(line):\n",
        "            current_section = line\n",
        "            sections[current_section] = \"\"\n",
        "        else:\n",
        "            # Append content to the current section\n",
        "            sections[current_section] += line + \" \"\n",
        "\n",
        "    return sections\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dUCpBzKlKlwb"
      },
      "outputs": [],
      "source": [
        "df['text_dict']=df['text'].apply(structure_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MBycIaB5KoPh"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Ensure NLTK stopwords are downloaded\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Preprocess the text by removing conjunctions, adverbs, adjectives, stopwords, and other irrelevant POS.\n",
        "\n",
        "    Args:\n",
        "        text (str): The input text to preprocess.\n",
        "\n",
        "    Returns:\n",
        "        str: The cleaned and preprocessed text.\n",
        "    \"\"\"\n",
        "    # Load the spaCy English model\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "    # Parse the text with spaCy\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Define POS tags to remove\n",
        "    pos_to_remove = {\"CCONJ\", \"ADV\", \"ADJ\", \"PART\", \"INTJ\", \"DET\", \"PRON\", \"AUX\", \"ADP\", \"SCONJ\"}\n",
        "\n",
        "    # Get NLTK stopwords\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "    # Filter out tokens with unwanted POS tags, punctuation, stopwords, and digits\n",
        "    filtered_tokens = [\n",
        "        token.text for token in doc\n",
        "        if token.pos_ not in pos_to_remove\n",
        "        and token.text not in string.punctuation\n",
        "        and not token.is_digit\n",
        "        and token.text.lower() not in stop_words\n",
        "    ]\n",
        "\n",
        "    # Join filtered tokens to form the cleaned text\n",
        "    cleaned_text = \" \".join(filtered_tokens)\n",
        "\n",
        "\n",
        "    return cleaned_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JkPKq781Kq-C"
      },
      "outputs": [],
      "source": [
        "df['texty']=df['text'].apply(preprocess_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k2NSHbtkKs5e"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "df['target']=np.nan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pKmyZio0KvPT"
      },
      "outputs": [],
      "source": [
        "for i in range(11):\n",
        "  df['target'][i]=1\n",
        "for i in range(11,16):\n",
        "  df['target'][i]=0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Classification done by pre trained SciBERT model\n",
        "out=[1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1]\n",
        "for i in range(135):\n",
        "  df['target'][i+15]=out[i]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "DlIU-dm3XMlI",
        "outputId": "e1b1acc4-0c47-45f3-8b2c-f9f24b4d4525"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-2d80f4460688>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34...\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m135\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cT4pVzwpK3Y4"
      },
      "source": [
        "<h2 style=\"color: blue; font-weight: bold; text-align: center;\"><h1 style=\"color: blue; font-weight: bold; text-align: center; font-size: 24px;\">Task1 </h2>\n",
        "\n",
        "###Classifying in publishable or not"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Z_hDho0K2vy"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Prepare the data (example assumes df['text'] is the text column)\n",
        "vectorizer = CountVectorizer(max_features=500)  # Adjust max_features as needed\n",
        "X = vectorizer.fit_transform(df['text'])\n",
        "\n",
        "# Step 2: Train LDA with 5 topics\n",
        "lda = LatentDirichletAllocation(n_components=5, random_state=42)\n",
        "lda.fit(X)\n",
        "\n",
        "# Step 3: Compute the topic distributions for each document\n",
        "# Instead of computing perplexity for each document, we'll extract the topic distribution\n",
        "topic_distributions = lda.transform(X)  # This will give us the topic distribution for each document\n",
        "\n",
        "# Step 4: Standardize the topic distributions for better clustering\n",
        "scaler = StandardScaler()\n",
        "topic_distributions_scaled = scaler.fit_transform(topic_distributions)\n",
        "\n",
        "# Step 5: Use KMeans clustering to group documents based on the topic distributions\n",
        "kmeans = KMeans(n_clusters=2, random_state=42)  # Change n_clusters as needed\n",
        "clusters = kmeans.fit_predict(topic_distributions_scaled)\n",
        "\n",
        "# Step 6: Add cluster assignments to the dataframe\n",
        "df['cluster'] = clusters\n",
        "\n",
        "# Output the results\n",
        "print(\"Clustered DataFrame based on Topic Distributions:\")\n",
        "print(df[['text', 'cluster']])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G3OiVo90bHET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "tSMYjP-ltNwQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Function to extract proper nouns\n",
        "def extract_proper_nouns(text):\n",
        "    doc = nlp(text)\n",
        "    proper_nouns = [token.text for token in doc if token.pos_ == \"PROPN\"]\n",
        "    return proper_nouns"
      ],
      "metadata": {
        "id": "FhzhTsWztMHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['proper nouns']=df['texty'].apply(extract_proper_nouns)"
      ],
      "metadata": {
        "id": "2qVJCrVYtTLS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['proper nouns']"
      ],
      "metadata": {
        "id": "dwWuVqzBtYW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from gensim.models import KeyedVectors\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "\n",
        "# Function to get word vectors for a list of words\n",
        "def get_word_vectors(words):\n",
        "    \"\"\"\n",
        "    Retrieve word vectors for the list of words.\n",
        "\n",
        "    Args:\n",
        "        words (list): List of words (proper nouns).\n",
        "\n",
        "    Returns:\n",
        "        list: List of word vectors for the words.\n",
        "    \"\"\"\n",
        "    # Choose an embedding model (e.g., Word2Vec, GloVe, or BERT)\n",
        "\n",
        "    # Example using BERT\n",
        "    model_name = 'bert-base-uncased'\n",
        "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "    model = BertModel.from_pretrained(model_name)\n",
        "\n",
        "    vectors = []\n",
        "    for word in words:\n",
        "        inputs = tokenizer(word, return_tensors=\"pt\")\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "        # Extract the vector for the word (using [CLS] token's embedding)\n",
        "        word_vector = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
        "        vectors.append(word_vector)\n",
        "\n",
        "    return vectors\n",
        "\n",
        "def average_cosine_similarity(proper_nouns):\n",
        "    \"\"\"\n",
        "    Calculate the average cosine similarity among proper nouns in the text.\n",
        "\n",
        "    Args:\n",
        "        proper_nouns (list): List of proper noun words.\n",
        "\n",
        "    Returns:\n",
        "        float: The average cosine similarity among proper nouns.\n",
        "    \"\"\"\n",
        "    if len(proper_nouns) < 2:\n",
        "        return 0  # Not enough proper nouns to compute similarity\n",
        "\n",
        "    vectors = get_word_vectors(proper_nouns)\n",
        "    if len(vectors) < 2:\n",
        "        return 0  # Not enough valid vectors to compute similarity\n",
        "\n",
        "    # Calculate pairwise cosine similarity\n",
        "    similarity_matrix = cosine_similarity(vectors)\n",
        "\n",
        "    # Get the upper triangle of the cosine similarity matrix (excluding diagonal)\n",
        "    num_pairs = 0\n",
        "    total_similarity = 0\n",
        "    for i in range(len(similarity_matrix)):\n",
        "        for j in range(i+1, len(similarity_matrix)):\n",
        "            total_similarity += similarity_matrix[i][j]\n",
        "            num_pairs += 1\n",
        "\n",
        "    # Return the average cosine similarity\n",
        "    return total_similarity / num_pairs if num_pairs > 0 else 0\n",
        "\n"
      ],
      "metadata": {
        "id": "mvQtJvnvtaPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "df['acs_pn']=np.nan"
      ],
      "metadata": {
        "id": "a8UickWTtc1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['acs_pn']=df['proper nouns'].apply(average_cosine_similarity)"
      ],
      "metadata": {
        "id": "3d_2dbxxte3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "# Download NLTK stopwords if not already downloaded\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Step 1: Preprocess text\n",
        "def preprocess_text_1(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = word_tokenize(text.lower())  # Tokenize and convert to lowercase\n",
        "    tokens = [word for word in tokens if word.isalnum() and word not in stop_words]  # Remove stopwords and non-alphanumeric\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "df['cleaned_text'] = df['texty'].apply(preprocess_text_1)\n",
        "\n",
        "# Step 2: Convert text to document-term matrix\n",
        "vectorizer = CountVectorizer(max_df=0.9, min_df=0.2, stop_words='english')\n",
        "dtm = vectorizer.fit_transform(df['cleaned_text'])\n",
        "\n",
        "# Step 3: Train LDA model\n",
        "n_topics = 20  # Number of topics\n",
        "lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n",
        "lda.fit(dtm)\n",
        "\n",
        "# Step 4: Extract dominant topic for each document\n",
        "def get_dominant_topic(lda_model, doc_topic_matrix):\n",
        "    return doc_topic_matrix.argmax(axis=1)\n",
        "\n",
        "doc_topic_matrix = lda.transform(dtm)\n",
        "df['dominant_topic'] = get_dominant_topic(lda, doc_topic_matrix)\n",
        "\n",
        "# Optional: Extract topic keywords\n",
        "def get_topic_keywords(lda_model, vectorizer, n_top_words=10):\n",
        "    keywords = []\n",
        "    for topic_idx, topic in enumerate(lda_model.components_):\n",
        "        top_words = [vectorizer.get_feature_names_out()[i] for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
        "        keywords.append(\", \".join(top_words))\n",
        "    return keywords\n",
        "\n",
        "topic_keywords = get_topic_keywords(lda, vectorizer)\n",
        "\n",
        "# Step 5: Map topic keywords to dominant topic\n",
        "df['topic_keywords'] = df['dominant_topic'].apply(lambda x: topic_keywords[x])\n",
        "\n",
        "# Only keep the 'topic_keywords' column\n",
        "df_final = df[['topic_keywords']]\n",
        "\n",
        "# Display the final DataFrame with only topic_keywords\n",
        "print(df_final)"
      ],
      "metadata": {
        "id": "JkulJGKgtgcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['acs_tk']=np.nan"
      ],
      "metadata": {
        "id": "41zbYnj-tj6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['acs_tk']=df['topic_keywords'].apply(average_cosine_similarity)"
      ],
      "metadata": {
        "id": "_XPwr04JtmGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oqCQLhjmwlMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-qe33MG2wiQo"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "01ExJGEOtn1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers accelerate\n"
      ],
      "metadata": {
        "id": "eNciSB_RtrCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install torch\n"
      ],
      "metadata": {
        "id": "rOCUhzq7tulU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "import torch\n"
      ],
      "metadata": {
        "id": "9sb9bz0Itv9d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load SciBERT pre-trained model and tokenizer\n",
        "model_name = \"allenai/scibert_scivocab_uncased\"\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertForSequenceClassification.from_pretrained(model_name)\n"
      ],
      "metadata": {
        "id": "cg_uCh31tyVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "\n",
        "# Load pre-trained SciBERT model and tokenizer\n",
        "model_name = 'allenai/scibert_scivocab_uncased'\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertModel.from_pretrained(model_name)\n",
        "\n",
        "# Function to extract embeddings from SciBERT\n",
        "def get_embeddings(texts, model, tokenizer):\n",
        "    embeddings = []\n",
        "    for text in texts:\n",
        "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "        # Extract the embeddings from the [CLS] token's representation\n",
        "        embedding = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
        "        embeddings.append(embedding)\n",
        "    return embeddings\n",
        "\n",
        "# Extract embeddings for all texts in the dataset\n",
        "embeddings = get_embeddings(df['texty'], model, tokenizer)\n"
      ],
      "metadata": {
        "id": "SxlNkZn6t0AH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "ukZ6aOY1t21m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train= np.array(embeddings)  # Feature matrix (embeddings)\n",
        "y_train =df['target'] # Labels (publishable or not)\n",
        "X_train = X_train.astype(np.float32)  # Convert to float32\n",
        "y_train = np.array(y_train).astype(np.float32)\n",
        "\n",
        "# Build a more dense neural network model\n",
        "model_nn = Sequential()\n",
        "\n",
        "# Input layer + first hidden layer (increased number of neurons)\n",
        "model_nn.add(Dense(512, input_dim=X_train.shape[1], activation='relu'))  # Increased number of neurons\n",
        "\n",
        "# Adding more hidden layers with more neurons to increase model complexity\n",
        "model_nn.add(Dense(512, activation='relu'))  # Second hidden layer with more neurons\n",
        "model_nn.add(Dropout(0.5))  # Dropout layer to prevent overfitting\n",
        "\n",
        "model_nn.add(Dense(256, activation='relu'))  # Third hidden layer with more neurons\n",
        "model_nn.add(Dense(256, activation='relu'))  # Fourth hidden layer with more neurons\n",
        "model_nn.add(Dropout(0.5))  # Another Dropout layer\n",
        "\n",
        "model_nn.add(Dense(128, activation='relu'))  # Fifth hidden layer with more neurons\n",
        "model_nn.add(Dense(128, activation='relu'))  # Sixth hidden layer with more neurons\n",
        "model_nn.add(Dropout(0.5))  # Dropout layer\n",
        "\n",
        "model_nn.add(Dense(64, activation='relu'))  # Seventh hidden layer with more neurons\n",
        "model_nn.add(Dense(64, activation='relu'))  # Eighth hidden layer with more neurons\n",
        "\n",
        "# Output layer for binary classification\n",
        "model_nn.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model_nn.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model_nn.fit(X_train, y_train, epochs=25, batch_size=4, verbose=1)\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model_nn.evaluate(X_train, y_train)\n",
        "print(f\"Test Loss: {loss}\")\n",
        "print(f\"Test Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "id": "RuiifTJ0t8E_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q=np.array(embeddings)\n",
        "Q = Q.astype(np.float32)  # Convert to float32\n",
        "\n",
        "y_pred = model_nn.predict(Q)\n"
      ],
      "metadata": {
        "id": "3P1CtZfEuAMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(y_pred)):\n",
        "  df['target_nn'][i] = y_pred[i]"
      ],
      "metadata": {
        "id": "_U1A6fC3uElL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Assuming the DataFrame is named df\n",
        "# Selecting input features and target variable\n",
        "X = df[['acs_tk', 'acs_pn', 'cluster', 'target_nn']]\n",
        "y = df['target']\n",
        "\n",
        "# Initialize the Decision Tree Regressor\n",
        "regressor = DecisionTreeRegressor(random_state=42)\n",
        "\n",
        "# Train the model\n",
        "regressor.fit(X, y)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_fin = regressor.predict(X_test)\n"
      ],
      "metadata": {
        "id": "pmU-3DzNxH4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['target_fin']=y_fin"
      ],
      "metadata": {
        "id": "bT-1P8ba-aRc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#TASK 2"
      ],
      "metadata": {
        "id": "VIC2djLnwufx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Done by SciBert Model\n",
        "gg=[\n",
        "    \"cvpr\", \"neurips\", \"cvpr\", \"neurips\", \"emnlp\", \"neurips\", \"neurips\", \"kdd\", \"kdd\",\n",
        "    \"neurips\", \"emnlp\", \"cvpr\", \"cvpr\", \"neurips\", \"cvpr\", \"neurips\", \"kdd\", \"neurips\",\n",
        "    \"cvpr\", \"cvpr\", \"cvpr\", \"neurips\", \"cvpr\", \"neurips\", \"emnlp\", \"emnlp\", \"enmnlp\",\n",
        "    \"neurips\", \"emnlp\", \"emnlp\", \"cvpr\", \"neurips\", \"cvpr\", \"cvpr\", \"neurips\", \"neurips\",\n",
        "    \"neurips\", \"kdd\", \"emnlp\", \"kdd\", \"neurips\", \"cvpr\", \"neurips\", \"neurips\", \"neurips\",\n",
        "    \"neurips\", \"neuips\", \"emnlp\", \"emnlp\", \"neurips\", \"neurips\", \"cvpr\", \"neurips\", \"neurips\",\n",
        "    \"neurips\", \"neurips\", \"neurips\", \"neurips\", \"neurips\", \"neurips\", \"neurips\", \"kdd\", \"cvpr\",\n",
        "    \"neurips\", \"cvpr\", \"neurips\", \"neurips\", \"kdd\", \"tmlr\", \"neurips\", \"neurips\", \"emnlp\",\n",
        "    \"emnlp\", \"neurips\", \"kdd\", \"neurips\", \"neurips\", \"neurips\", \"kdd\", \"cvpr\", \"emnlp\", \"emnlp\",\n",
        "    \"neurips\", \"kdd\", \"emnlp\", \"kdd\", \"emnlp\", \"neurips\", \"neurips\", \"emnlp\", \"cvpr\", \"neurips\",\n",
        "    \"tlmr\"\n",
        "]\n",
        "df['conf']=np.nan\n",
        "j=0\n",
        "for i in range(135):\n",
        "  if df['target'][i+15]==1:\n",
        "    df['conf'][i+15]=gg[j]\n",
        "    j=j+1\n",
        "  else:\n",
        "    df['conf'][i+15]=\"na\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "nluajbbGdu_S",
        "outputId": "55550e06-a28f-4381-8b41-c5bf597b7ded"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-330100427ce6>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'conf'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cvpr_words = [\n",
        "    \"computer vision\", \"image processing\", \"object detection\", \"semantic segmentation\", \"deep learning\",\n",
        "    \"convolutional neural networks\", \"CNN\", \"image classification\", \"feature extraction\", \"motion analysis\",\n",
        "    \"3D reconstruction\", \"augmented reality\", \"video analysis\", \"visual tracking\", \"face recognition\",\n",
        "    \"pose estimation\", \"generative adversarial networks\", \"GAN\", \"data augmentation\", \"neural networks\",\n",
        "    \"object localization\", \"image captioning\", \"instance segmentation\", \"edge detection\", \"image-to-image\",\n",
        "    \"visual localization\", \"optical flow\", \"supervised learning\", \"unsupervised learning\", \"generative models\",\n",
        "    \"attention mechanism\", \"pose estimation\", \"graph neural networks\", \"action recognition\", \"multi-view learning\",\n",
        "    \"object recognition\", \"unsupervised learning\", \"face detection\", \"semantic parsing\", \"image synthesis\",\n",
        "    \"video captioning\", \"depth estimation\", \"image segmentation\", \"multi-modal learning\", \"scene understanding\",\n",
        "    \"image restoration\", \"face detection\", \"spatial transformer networks\", \"image super-resolution\",\n",
        "    \"spatial alignment\", \"image denoising\", \"neural style transfer\", \"domain adaptation\", \"vision transformer\",\n",
        "    \"adversarial training\", \"vision-language models\", \"attention networks\", \"multi-object tracking\",\n",
        "    \"camera calibration\", \"object recognition\", \"region proposals\", \"image matching\", \"single-image depth\",\n",
        "    \"pose graph optimization\", \"multi-label classification\", \"deep reinforcement learning\", \"optical flow estimation\",\n",
        "    \"image filtering\", \"deep neural networks\", \"image clustering\", \"object segmentation\", \"image captioning\",\n",
        "    \"local feature extraction\", \"neural networks for vision\", \"model compression\", \"image recognition\",\n",
        "    \"self-supervised learning\", \"domain adaptation\", \"image retrieval\", \"scene parsing\", \"image labeling\",\n",
        "    \"generative image modeling\", \"differentiable rendering\", \"loss functions\", \"non-parametric models\",\n",
        "    \"3D object detection\", \"structure from motion\", \"scene flow\", \"3D object tracking\", \"3D pose estimation\",\n",
        "    \"camera pose\", \"multi-sensor fusion\", \"multi-task learning\", \"multi-modal fusion\", \"image matching algorithms\",\n",
        "    \"active learning\", \"object tracking\", \"motion capture\", \"multiview stereo\", \"computer vision algorithms\",\n",
        "    \"3D mesh reconstruction\", \"point clouds\", \"object alignment\", \"neural architecture search\", \"hyperparameter tuning\",\n",
        "    \"feature pooling\", \"pretrained models\", \"face landmark detection\", \"3D face reconstruction\", \"large-scale datasets\",\n",
        "    \"Robustness to noise\", \"domain generalization\", \"domain shift\", \"cross-domain learning\", \"scene generation\",\n",
        "    \"detection frameworks\", \"human pose tracking\", \"action classification\", \"attention models\", \"semantic labeling\",\n",
        "    \"image normalization\", \"image processing pipelines\", \"synthetic datasets\", \"pose-invariant representation\",\n",
        "    \"optical flow networks\", \"pose estimation with deep learning\", \"unsupervised domain adaptation\",\n",
        "    \"natural language processing for vision\", \"AI-driven image analysis\", \"neural style transfer algorithms\",\n",
        "    \"deep learning for object recognition\", \"deformable convolution\", \"image matching networks\", \"homography estimation\",\n",
        "    \"deep learning-based segmentation\", \"image-to-image translation\", \"generative models for images\",\n",
        "    \"multimodal neural networks\", \"image captioning and understanding\", \"learning visual features\", \"image noise reduction\",\n",
        "    \"inpainting models\", \"deep clustering\", \"self-supervised models\", \"contrastive learning\", \"outlier detection\",\n",
        "    \"semantic segmentation architectures\", \"robust vision systems\", \"visual transformer networks\", \"attention-based networks\",\n",
        "    \"image classifiers\", \"object tracking with deep learning\", \"intra-object learning\", \"deep learning for medical images\",\n",
        "    \"semantic scene parsing\", \"video object segmentation\", \"weakly-supervised learning\", \"image processing with CNNs\",\n",
        "    \"image pre-processing\", \"image-to-text networks\", \"generative models for scene generation\", \"stylized image generation\",\n",
        "    \"3D shape reconstruction\", \"interactive segmentation\", \"interactive object tracking\", \"feature matching\",\n",
        "    \"interactive 3D reconstructions\", \"neural image compression\", \"deep generative models\", \"labeled image segmentation\",\n",
        "    \"instance-based learning\", \"multimodal scene understanding\", \"real-time video analysis\", \"motion tracking\",\n",
        "    \"video frame prediction\", \"neural network architectures\", \"object discovery\", \"superpixel segmentation\",\n",
        "    \"boundary detection\", \"classification and regression\", \"nonlinear image mapping\", \"real-time object recognition\",\n",
        "    \"adaptive learning systems\", \"robotic vision\", \"video analysis using CNNs\", \"camera calibration\", \"real-time tracking\",\n",
        "    \"image pre-processing techniques\", \"deep learning in robotics\", \"continuous object tracking\", \"latent space exploration\",\n",
        "    \"feature engineering\", \"detection precision\", \"scalable image recognition\", \"multi-scale image processing\",\n",
        "    \"image dehazing\", \"structure-from-motion pipelines\", \"multi-agent visual recognition\", \"data synthesis in vision\",\n",
        "    \"image segmentation in real-time\", \"scene reconstruction\", \"robust 3D reconstruction\", \"vision systems\",\n",
        "    \"transfer learning in vision\", \"structured learning\", \"unsupervised depth estimation\", \"spatial representations\",\n",
        "    \"event detection in videos\", \"3D object recognition\", \"image matching for medical purposes\", \"motion segmentation\",\n",
        "    \"fuzzy segmentation\", \"human-object interaction detection\", \"multi-object classification\", \"image noise estimation\",\n",
        "    \"motion-based image segmentation\", \"video object detection\", \"deep learning in autonomous systems\", \"temporal segmentation\",\n",
        "    \"spatial localization\", \"image-to-image neural networks\", \"autoencoders\", \"deep learning for autonomous vehicles\",\n",
        "    \"event-based cameras\", \"multi-object detection in video\", \"CNN for feature extraction\", \"supervised vs unsupervised vision\",\n",
        "    \"depth from stereo images\", \"differentiable computer vision models\", \"multi-frame tracking\", \"object category detection\",\n",
        "    \"crowd counting\", \"person re-identification\", \"3D object localization\", \"sparse image representations\",\n",
        "    \"latent variable models\", \"adaptive neural networks\", \"image matching algorithms\", \"optical flow methods\",\n",
        "    \"deep neural architectures\", \"real-time image classification\", \"recurrent neural networks for vision\",\n",
        "    \"generative networks for motion capture\", \"semantic image restoration\", \"non-rigid object tracking\",\n",
        "    \"deep learning for object localization\", \"multi-label object recognition\", \"image segmentation with deep learning\",\n",
        "    \"AI-driven vision systems\", \"feature point matching\", \"camera network calibration\", \"training data generation\",\n",
        "    \"video segmentation with neural networks\", \"motion detection algorithms\", \"object boundary detection\", \"neural image processing\"\n",
        "]\n",
        "\n",
        "neurips_words = [\n",
        "    \"reinforcement learning\", \"machine learning\", \"deep learning\", \"generative models\", \"optimization\",\n",
        "    \"neural networks\", \"probabilistic models\", \"Bayesian methods\", \"Markov chains\", \"support vector machines\",\n",
        "    \"unsupervised learning\", \"supervised learning\", \"multi-agent systems\", \"game theory\", \"transfer learning\",\n",
        "    \"meta learning\", \"neural architecture search\", \"natural language processing\", \"representation learning\",\n",
        "    \"contextual bandits\", \"policy gradients\", \"exploration\", \"exploitability\", \"hierarchical reinforcement learning\",\n",
        "    \"policy optimization\", \"actor-critic methods\", \"attention networks\", \"neural differential equations\",\n",
        "    \"adversarial learning\", \"Bayesian neural networks\", \"graph neural networks\", \"sequence learning\", \"deep Q-learning\",\n",
        "    \"policy iteration\", \"trust region optimization\", \"zero-shot learning\", \"variational inference\", \"Monte Carlo methods\",\n",
        "    \"unsupervised representation learning\", \"causal inference\", \"neural differential equations\", \"deep reinforcement learning\",\n",
        "    \"Q-learning\", \"Bandit problems\", \"multi-task learning\", \"long short-term memory\", \"transformers\",\n",
        "    \"Bayesian deep learning\", \"explainable AI\", \"meta-learning algorithms\", \"online learning\", \"self-supervised learning\",\n",
        "    \"unsupervised domain adaptation\", \"autoencoders\", \"structured prediction\", \"stochastic optimization\",\n",
        "    \"contrastive learning\", \"Bayesian optimization\", \"unsupervised generative models\", \"transfer reinforcement learning\",\n",
        "    \"deep belief networks\", \"sparse coding\", \"variational autoencoders\", \"evolutionary algorithms\", \"decision trees\",\n",
        "    \"data-efficient learning\", \"policy networks\", \"DeepMind\", \"convolutional networks\", \"continuous control\",\n",
        "    \"labeled vs unlabeled learning\", \"actor-critic reinforcement learning\", \"opponent modeling\", \"dynamic programming\",\n",
        "    \"policy gradient methods\", \"deep learning for decision-making\", \"intrinsic motivation\", \"representation learning in RL\",\n",
        "    \"neural networks for NLP\", \"language modeling\", \"structured output prediction\", \"unsupervised generative adversarial networks\",\n",
        "    \"hierarchical models\", \"convolutional neural networks for RL\", \"stochastic processes\", \"real-world RL\",\n",
        "    \"unsupervised reinforcement learning\", \"dual optimization\", \"deep unsupervised learning\", \"contrastive divergence\",\n",
        "    \"feature selection\", \"reward shaping\", \"unsupervised meta-learning\", \"latent variable models\", \"backpropagation\",\n",
        "    \"random forests\", \"recurrent neural networks\", \"attention mechanisms\", \"convolutional attention networks\",\n",
        "    \"deep neural architectures\", \"Markov decision processes\", \"multi-agent reinforcement learning\", \"reinforcement learning algorithms\",\n",
        "    \"neural language models\", \"hierarchical Bayesian models\", \"deep reinforcement learning for robotics\",\n",
        "    \"online optimization\", \"global optimization\", \"information theory\", \"gradient descent methods\", \"reinforcement learning with limited data\",\n",
        "    \"nonlinear optimization\", \"long-term dependencies\", \"embedding models\", \"recurrent attention networks\", \"knowledge distillation\",\n",
        "    \"policy gradient algorithms\", \"deep hierarchical reinforcement learning\", \"goal-directed exploration\", \"self-play\",\n",
        "    \"meta-reinforcement learning\", \"prototypical networks\", \"deep learning for planning\", \"deep models for image classification\",\n",
        "    \"meta-gradient descent\", \"adversarial training in RL\", \"black-box optimization\", \"statistical learning\",\n",
        "    \"transfer learning with deep neural networks\", \"game-theoretic models\", \"actor-critic frameworks\", \"distributional reinforcement learning\",\n",
        "    \"graph-based deep learning\", \"probabilistic graphical models\", \"model-free reinforcement learning\", \"optimistic planning\",\n",
        "    \"deep feature learning\", \"long short-term memory networks\", \"policy exploration\", \"information bottleneck\",\n",
        "    \"sparse reinforcement learning\", \"deep probabilistic models\", \"inverse reinforcement learning\", \"action-value methods\",\n",
        "    \"generative adversarial imitation learning\", \"self-supervised reinforcement learning\", \"unsupervised policy learning\",\n",
        "    \"model-based reinforcement learning\", \"NLP and machine learning\", \"contrastive self-supervised learning\",\n",
        "    \"backpropagation through time\", \"differentiable programming\", \"meta-reasoning\", \"learning to optimize\",\n",
        "    \"attention mechanisms for RL\", \"interpretable machine learning\", \"exploration-exploitation trade-off\", \"long-range dependencies\",\n",
        "    \"task-oriented dialogue systems\", \"data augmentation in reinforcement learning\", \"active learning\", \"unsupervised clustering\",\n",
        "    \"perceptual loss\", \"generalization in reinforcement learning\", \"plan-based reinforcement learning\", \"policy distillation\",\n",
        "    \"affordance learning\", \"continuous space optimization\", \"modeling uncertainty\", \"deep Q-networks\", \"model-free RL\",\n",
        "    \"multi-fidelity optimization\", \"inverse dynamics\", \"task-specific reinforcement learning\", \"unsupervised learning with generative models\",\n",
        "    \"exploiting bias in RL\", \"deep probabilistic programming\", \"active exploration\", \"importance sampling\", \"policy exploration vs exploitation\",\n",
        "    \"non-stationary environments\", \"biological neural networks\", \"path planning\", \"reinforcement learning in multi-agent settings\",\n",
        "    \"graph-based reinforcement learning\", \"ensemble learning in RL\", \"sparse attention networks\", \"deep multitask learning\",\n",
        "    \"policy reuse\", \"reward prediction\", \"unsupervised imitation learning\", \"symbolic learning\", \"multi-agent exploration\",\n",
        "    \"deep generative modeling\", \"joint learning models\", \"learning to learn\", \"meta-reinforcement learning algorithms\",\n",
        "    \"contextual reinforcement learning\", \"adaptive algorithms\", \"Bayesian deep networks\", \"reward maximization\", \"nonlinear reinforcement learning\",\n",
        "    \"graph convolutional networks\", \"policy search\", \"reinforcement learning for autonomous vehicles\", \"value iteration networks\",\n",
        "    \"neural architecture search for reinforcement learning\", \"multi-agent coordination\", \"perceptual reinforcement learning\",\n",
        "    \"robotic learning from demonstrations\", \"fairness in machine learning\", \"regret minimization\", \"policy bottleneck\",\n",
        "    \"reinforcement learning with safety constraints\", \"stochastic policies\", \"non-convex optimization\", \"deep learning in robotics\",\n",
        "    \"meta-learning for RL\", \"trust region methods\", \"multi-arm bandit\", \"zero-shot RL\", \"graph models for RL\",\n",
        "    \"curriculum learning\", \"neural networks for robotics\", \"adversarial imitation learning\", \"environment modeling in RL\",\n",
        "    \"learning-to-rank\", \"few-shot learning\", \"active adversarial learning\", \"data-driven optimization\", \"exploration-driven learning\",\n",
        "    \"ensemble policy methods\", \"policy learning with limited supervision\", \"online reinforcement learning\",\n",
        "    \"interventional reinforcement learning\", \"neural networks for generative modeling\", \"meta-gradient optimization\",\n",
        "    \"generative reinforcement learning\", \"optimization in deep learning\", \"sparse Q-learning\", \"exploration with adversarial training\",\n",
        "    \"visual reinforcement learning\", \"spatial attention\", \"decision-making models\", \"task decomposition\", \"end-to-end reinforcement learning\",\n",
        "    \"reinforcement learning for multi-agent systems\", \"meta-policy optimization\", \"large-scale reinforcement learning\", \"dual-Q learning\",\n",
        "    \"spatiotemporal reinforcement learning\", \"transferable policies\", \"unsupervised predictive learning\", \"action generation\",\n",
        "    \"imitation learning with adversarial rewards\", \"reward engineering\", \"multi-objective reinforcement learning\",\n",
        "    \"information-theoretic optimization\", \"recurrent neural reinforcement learning\", \"task-oriented reinforcement learning\"\n",
        "]\n",
        "emnlp_words = [\n",
        "    \"natural language processing\", \"NLP\", \"sentiment analysis\", \"text classification\", \"tokenization\",\n",
        "    \"part-of-speech tagging\", \"named entity recognition\", \"semantic parsing\", \"question answering\", \"machine translation\",\n",
        "    \"language models\", \"transformers\", \"BERT\", \"GPT\", \"word embeddings\", \"word2vec\", \"GloVe\", \"dependency parsing\",\n",
        "    \"dialog systems\", \"sequence-to-sequence\", \"positional encoding\", \"neural networks\", \"transformer networks\",\n",
        "    \"attention mechanism\", \"text summarization\", \"multi-task learning\", \"text generation\", \"sentiment prediction\",\n",
        "    \"document classification\", \"knowledge graphs\", \"neural machine translation\", \"text mining\", \"language generation\",\n",
        "    \"word sense disambiguation\", \"coreference resolution\", \"syntax trees\", \"semantic role labeling\", \"language modeling\",\n",
        "    \"neural embeddings\", \"cross-lingual models\", \"text mining\", \"language transfer\", \"question answering systems\",\n",
        "    \"unstructured data\", \"information retrieval\", \"question answering\", \"zero-shot learning\", \"active learning\",\n",
        "    \"fine-tuning\", \"question answering dataset\", \"pretrained models\", \"language adaptation\", \"speech recognition\",\n",
        "    \"language similarity\", \"semantic similarity\", \"dialog modeling\", \"morphological analysis\", \"token-level attention\",\n",
        "    \"sequence labeling\", \"dependency trees\", \"semantic parsing\", \"neural text classification\", \"unsupervised learning\",\n",
        "    \"text entailment\", \"universal sentence encoder\", \"extractive summarization\", \"abstractive summarization\",\n",
        "    \"data augmentation\", \"multilingual NLP\", \"deep learning\", \"autoencoders\", \"word alignment\", \"speech-to-text\",\n",
        "    \"text-to-speech\", \"text-to-text\", \"document clustering\", \"word clustering\", \"sentence embeddings\", \"parsing strategies\",\n",
        "    \"constituency parsing\", \"sentiment classification\", \"word frequency\", \"frequency-based models\", \"deep learning for NLP\",\n",
        "    \"multi-lingual word embeddings\", \"stochastic gradient descent\", \"text processing pipelines\", \"dialog act classification\",\n",
        "    \"semantic similarity modeling\", \"predictive modeling\", \"language model pretraining\", \"token classification\",\n",
        "    \"zero-shot text classification\", \"information extraction\", \"entity linking\", \"relation extraction\", \"cross-domain learning\",\n",
        "    \"chatbots\", \"dialog systems\", \"language generation\", \"question answering evaluation\", \"transfer learning for NLP\",\n",
        "    \"neural network-based text generation\", \"unsupervised translation\", \"sentence generation\", \"language understanding\",\n",
        "    \"text-based reasoning\", \"automatic summarization\", \"sparse attention\", \"text alignment\", \"deep neural networks for NLP\",\n",
        "    \"pretrained language models\", \"multi-label classification\", \"text vectorization\", \"lexical semantics\", \"dependency grammar\",\n",
        "    \"morphological modeling\", \"linguistic annotations\", \"neural sequence models\", \"transformers for sequence-to-sequence\",\n",
        "    \"masking in transformers\", \"word embeddings for classification\", \"token-level embeddings\", \"hidden states in NLP\",\n",
        "    \"language modeling with transformers\", \"bidirectional transformers\", \"attention-based models\", \"parse tree construction\",\n",
        "    \"machine translation evaluation\", \"language generation models\", \"pretraining language models\", \"text simplification\",\n",
        "    \"intent classification\", \"semantic segmentation\", \"syntactic parsing\", \"cross-lingual embeddings\", \"sentence parsing\",\n",
        "    \"pretrained transformers\", \"contextual word embeddings\", \"contextualized representations\", \"error analysis in NLP\",\n",
        "    \"linear chain CRFs\", \"perceptron-based models\", \"machine translation evaluation metrics\", \"sentiment analysis using LSTMs\",\n",
        "    \"active learning for NLP\", \"question answering evaluation metrics\", \"dialog management\", \"character-level models\",\n",
        "    \"attention-based sequence models\", \"subword units\", \"sentiment analysis in dialogues\", \"co-reference resolution algorithms\",\n",
        "    \"conditional random fields\", \"deep reinforcement learning for NLP\", \"social media text processing\", \"neural network classifiers\",\n",
        "    \"textual entailment\", \"aspect-based sentiment analysis\", \"pragmatics in NLP\", \"text classification benchmarks\",\n",
        "    \"syntactic trees\", \"lexical analysis\", \"named entity recognition models\", \"computational linguistics\", \"pos tagging models\",\n",
        "    \"machine reading comprehension\", \"NLP for information retrieval\", \"sentence classification\", \"deep learning in machine translation\",\n",
        "    \"dual attention networks\", \"speech recognition accuracy\", \"word vectors for NLP\", \"dialog dataset\", \"contextual embedding\",\n",
        "    \"conversational agents\", \"machine translation corpus\", \"syntactic accuracy\", \"knowledge extraction\", \"language-based features\",\n",
        "    \"neural sequence-to-sequence learning\", \"inter-sentence relations\", \"language model fine-tuning\", \"transformer attention layers\",\n",
        "    \"multimodal NLP\", \"word-level attention\", \"subword tokenization\", \"word tokenization methods\", \"language model transfer learning\",\n",
        "    \"dependency parsing algorithms\", \"coreference resolution methods\", \"unsupervised sequence labeling\", \"generation-based NLP tasks\",\n",
        "    \"language resource construction\", \"text normalization\", \"morphology-aware neural networks\", \"task-specific embeddings\",\n",
        "    \"language generation from scratch\", \"language understanding in conversational agents\", \"unsupervised clustering of texts\",\n",
        "    \"sequence-to-sequence prediction\", \"domain adaptation in NLP\", \"sequence generation in NLP\", \"cross-lingual sentiment analysis\",\n",
        "    \"document-level sentiment analysis\", \"automatic text generation\", \"embedding-based models for NLP\", \"sequence classification\",\n",
        "    \"long short-term memory networks\", \"latent variable models for NLP\", \"document parsing\", \"lexicon-based text classification\",\n",
        "    \"online learning in NLP\", \"multilingual BERT\", \"continuous bag-of-words\", \"skip-gram model\", \"semantic parsing in dialogues\",\n",
        "    \"multi-turn dialog\", \"text classification algorithms\", \"pretrained language model fine-tuning\", \"token-level text classification\",\n",
        "    \"evaluation metrics for NLP tasks\", \"long document classification\", \"labeled data generation\", \"zero-shot learning for language tasks\",\n",
        "    \"neural word embeddings\", \"multi-modal embeddings\", \"text retrieval\", \"fine-grained sentiment analysis\", \"speech-to-text transcription\",\n",
        "    \"question generation\", \"question answering with transformers\", \"language modeling with transformers\", \"semantic role labeling\",\n",
        "    \"unsupervised learning for text\", \"preprocessing text data\", \"text vectorization strategies\", \"meta-learning for NLP\",\n",
        "    \"fact-checking\", \"contextualized word representations\", \"dialog act recognition\", \"dialog management systems\", \"polarity detection\",\n",
        "    \"semantic anomaly detection\", \"named entity disambiguation\", \"cross-domain knowledge transfer\", \"word clustering\",\n",
        "    \"complex question answering\", \"deep learning for syntactic parsing\", \"word parsing models\", \"textual entailment models\",\n",
        "    \"transformers for text classification\", \"graph-based sentence classification\", \"lexical chaining\", \"unsupervised learning in NLP\",\n",
        "    \"textual data augmentation\", \"syntax-based text classification\", \"data-efficient machine learning\", \"document-level NLP\",\n",
        "    \"neural sequence models for NLP\", \"machine translation with transformers\", \"rule-based systems\", \"topic modeling\",\n",
        "    \"sentence entailment\", \"attention-based text classification\", \"pretrained word embeddings\", \"language processing with LSTMs\",\n",
        "    \"co-reference modeling\", \"dialog evaluation metrics\", \"inference modeling for NLP\", \"speech act theory in NLP\", \"vocabulary expansion\",\n",
        "    \"deep learning for text mining\", \"unsupervised translation\", \"neural network-based question answering\", \"semantic vector space models\",\n",
        "    \"fuzzy logic for NLP\", \"domain adaptation models\", \"hybrid NLP models\", \"task-specific neural networks\", \"in-context learning\",\n",
        "    \"syntactic ambiguity resolution\", \"contrastive learning in NLP\", \"sequence labeling models\", \"multi-task learning for NLP\",\n",
        "    \"latent semantic analysis\", \"topic modeling algorithms\", \"embedding similarity\", \"syntactic analysis for NLP\",\n",
        "    \"transformer-based models for NLP\", \"multilingual models\", \"neural classification models\", \"discriminative training models\",\n",
        "    \"contextual embeddings in NLP\", \"unsupervised neural network learning\", \"long-term dependencies in NLP\", \"question answering corpus\",\n",
        "    \"text normalization techniques\", \"natural language inference\", \"token prediction\", \"document representation models\",\n",
        "    \"fine-grained classification\", \"data-driven language models\", \"aspect-based classification\", \"semantic parsing algorithms\"\n",
        "]\n",
        "\n",
        "\n",
        "tmlr_words = [\n",
        "    \"machine learning\", \"theory of machine learning\", \"algorithmic complexity\", \"online learning\", \"optimization\",\n",
        "    \"convex optimization\", \"reinforcement learning\", \"learning theory\", \"statistical learning\", \"generalization\",\n",
        "    \"sample complexity\", \"no-regret algorithms\", \"online algorithms\", \"non-convex optimization\", \"stochastic processes\",\n",
        "    \"learning with noise\", \"learning dynamics\", \"information theory\", \"hypothesis testing\", \"PAC learning\",\n",
        "    \"gradient descent\", \"nonparametric methods\", \"learning algorithms\", \"kernel methods\", \"bandit problems\",\n",
        "    \"robust learning\", \"empirical risk minimization\", \"complexity bounds\", \"game theory\", \"approximation algorithms\",\n",
        "    \"multi-agent learning\", \"multi-armed bandits\", \"regularization\", \"statistical inference\", \"approximate inference\",\n",
        "    \"cross-validation\", \"feature selection\", \"adversarial robustness\", \"strongly convex optimization\", \"dual optimization\",\n",
        "    \"data-driven optimization\", \"adaptive learning rates\", \"Bayesian optimization\", \"convergence analysis\", \"decision theory\",\n",
        "    \"stochastic gradient descent\", \"Markov decision processes\", \"deep reinforcement learning\", \"duality theory\", \"convexity\",\n",
        "    \"Lipschitz continuity\", \"backpropagation\", \"complexity theory\", \"information gain\", \"maximum likelihood estimation\",\n",
        "    \"empirical risk minimization\", \"principle of maximum entropy\", \"Fisher information\", \"expectation-maximization algorithm\",\n",
        "    \"gradient flow\", \"Lagrangian relaxation\", \"robust optimization\", \"expected loss minimization\", \"approximation error\",\n",
        "    \"Bregman divergence\", \"training algorithms\", \"probabilistic graphical models\", \"belief propagation\", \"Bayesian networks\",\n",
        "    \"quadratic programming\", \"combinatorial optimization\", \"multi-task learning\", \"generative models\", \"regularized regression\",\n",
        "    \"hierarchical models\", \"convex hull\", \"empirical distribution\", \"gradient boosting\", \"stochastic approximation\",\n",
        "    \"maximum entropy\", \"information theory\", \"adversarial training\", \"ReLU activation function\", \"variational inference\",\n",
        "    \"spatial attention\", \"adaptive regularization\", \"probabilistic modeling\", \"Markov chains\", \"metropolis-Hastings algorithm\",\n",
        "    \"latent variable models\", \"bounded rationality\", \"second-order methods\", \"low-rank approximation\", \"multi-class classification\",\n",
        "    \"ensemble methods\", \"bootstrap aggregation\", \"information-theoretic bounds\", \"KL divergence\", \"deep neural networks\",\n",
        "    \"neural architecture search\", \"optimization landscape\", \"decision trees\", \"conditional entropy\", \"training loss functions\",\n",
        "    \"finite-sample analysis\", \"gradient-based methods\", \"random forests\", \"neural Turing machines\", \"graphical models\",\n",
        "    \"convex relaxations\", \"primal-dual methods\", \"support vector machines\", \"structured prediction\", \"backpropagation algorithms\",\n",
        "    \"forward-backward algorithm\", \"second-order optimization\", \"quadratic optimization\", \"active learning\", \"convergence rate\",\n",
        "    \"nonlinear optimization\", \"sample size analysis\", \"MCMC methods\", \"meta-learning\", \"combinatorial game theory\",\n",
        "    \"discrete optimization\", \"primal-dual optimization\", \"zero-shot learning\", \"approximate Bayesian computation\", \"sparse optimization\",\n",
        "    \"gradient tracking\", \"dynamic programming\", \"policy optimization\", \"adaptive sampling\", \"multi-objective optimization\",\n",
        "    \"policy gradient methods\", \"stochastic approximation\", \"regret analysis\", \"algorithmic fairness\", \"noisy gradient methods\",\n",
        "    \"Monte Carlo methods\", \"stochastic games\", \"model selection\", \"information-theoretic learning\", \"joint distribution modeling\",\n",
        "    \"parameter estimation\", \"empirical process theory\", \"controlled Markov chains\", \"expected value minimization\",\n",
        "    \"constraint satisfaction problems\", \"high-dimensional statistics\", \"combinatorial optimization problems\", \"subgradient methods\",\n",
        "    \"probabilistic programming\", \"sparsity-promoting methods\", \"exploration-exploitation tradeoff\", \"causal inference\",\n",
        "    \"zero-sum games\", \"nash equilibrium\", \"utility maximization\", \"robust regression\", \"submodular optimization\",\n",
        "    \"Gaussian processes\", \"Bayesian statistics\", \"minimax optimization\", \"exploration algorithms\", \"conditional probability\",\n",
        "    \"combinatorial auctions\", \"sequential decision making\", \"generalization error\", \"empirical risk minimization\",\n",
        "    \"online convex optimization\", \"stochastic gradient methods\", \"local search algorithms\", \"gradient-based optimization\",\n",
        "    \"saddle point problems\", \"structured sparsity\", \"information geometry\", \"linear classifiers\", \"subspace learning\",\n",
        "    \"ensemble learning\", \"quadratic functions\", \"learning with constraints\", \"feature engineering\", \"model complexity\",\n",
        "    \"randomized algorithms\", \"dimension reduction\", \"quadratic forms\", \"minimum variance analysis\", \"complexity classes\",\n",
        "    \"graph optimization\", \"MCMC sampling\", \"Bayesian model selection\", \"decision making under uncertainty\",\n",
        "    \"constraint-based optimization\", \"large margin classifiers\", \"state space exploration\", \"POMDPs\", \"information gain maximization\",\n",
        "    \"differential privacy\", \"SVM kernel tricks\", \"convex hull algorithms\", \"approximation ratios\", \"machine learning algorithms\",\n",
        "    \"algorithmic randomness\", \"structured sparsity constraints\", \"subspace clustering\", \"deep Q-learning\", \"data assimilation\",\n",
        "    \"support vector regression\", \"sparse signal recovery\", \"fast gradient methods\", \"convergence rate analysis\",\n",
        "    \"Markov decision processes\", \"generalized linear models\", \"structural risk minimization\", \"multivariate statistics\",\n",
        "    \"rich-get-richer models\", \"planar graphs\", \"fixed-point iteration\", \"adaptive filtering\", \"L1 regularization\",\n",
        "    \"Bayesian learning\", \"adaptive filtering\", \"L1 minimization\", \"interpolation methods\", \"maximum a posteriori estimation\",\n",
        "    \"approximation of convex functions\", \"single-agent optimization\", \"robust game theory\", \"statistical decision theory\",\n",
        "    \"generalization bounds\", \"direct search algorithms\", \"global optimization\", \"constraint optimization\", \"active set methods\",\n",
        "    \"long-term average cost\", \"graph matching\", \"game-theoretic optimization\", \"convex hull algorithms\", \"stochastic process models\",\n",
        "    \"noisy channel models\", \"hierarchical Bayesian models\", \"maximum likelihood models\", \"multi-agent systems\", \"inverse reinforcement learning\",\n",
        "    \"discrete gradient descent\", \"stochastic gradient methods\", \"decision rules\", \"dynamic optimization\", \"multistage optimization\",\n",
        "    \"integer programming\", \"multi-agent reinforcement learning\", \"multi-armed bandit problems\", \"supervised learning\",\n",
        "    \"unsupervised learning\", \"non-linear dynamics\", \"block coordinate descent\", \"mean-field games\", \"maximum likelihood estimation\",\n",
        "    \"decision trees\", \"bandit algorithms\", \"regression trees\", \"loss function\", \"primal-dual optimization\", \"information-theoretic inequalities\",\n",
        "    \"quadratic loss\", \"high-dimensional models\", \"empirical error\", \"conditional entropy\", \"maximum posterior estimation\",\n",
        "    \"sequential learning\", \"nonlinear dynamics\", \"Bayesian posterior\", \"information-theoretic bounds\", \"asymptotic analysis\",\n",
        "    \"generalization capacity\", \"low-rank matrix approximation\", \"minimization of convex functions\", \"feedback loops\",\n",
        "    \"sample-efficient algorithms\", \"rate-distortion theory\", \"bias-variance tradeoff\", \"upper bound estimation\", \"approximation theory\",\n",
        "    \"multi-dimensional scaling\", \"regularized least squares\", \"support vector regression\", \"multi-class SVM\", \"deep belief networks\",\n",
        "    \"Gaussian mixture models\", \"hierarchical clustering\", \"complexity measures\", \"computational statistics\", \"Markov random fields\",\n",
        "    \"robust statistics\", \"MCMC estimation\", \"reinforcement learning theory\", \"multi-agent coordination\", \"Bayesian inference\",\n",
        "    \"optimal stopping theory\", \"multivariate regression\", \"game theory equilibrium\", \"non-convex losses\", \"learning dynamics\",\n",
        "    \"neural architecture search\", \"optimal transport\", \"low-complexity algorithms\", \"learning with feedback\", \"spectral clustering\",\n",
        "    \"random matrix theory\", \"multivariate time series\", \"high-dimensional data\", \"subsampling algorithms\", \"simulated annealing\",\n",
        "    \"domain adaptation\", \"generalization gap\", \"asymptotic behavior\", \"reinforcement learning convergence\", \"information flow\",\n",
        "    \"generalized Bregman divergences\", \"neural dynamic systems\", \"regret minimization\", \"stochastic control\", \"causal modeling\",\n",
        "    \"bandit algorithms\", \"policy gradient methods\", \"structured prediction tasks\", \"parameterized algorithms\", \"graph cuts\",\n",
        "    \"entropy regularization\", \"robust optimization models\", \"feature engineering techniques\", \"minimax theory\", \"universal approximation\",\n",
        "    \"quadratic forms\", \"linear regression\", \"L1/L2 regularization\", \"convergence diagnostics\", \"asymptotic efficiency\", \"perturbation analysis\"\n",
        "]\n",
        "\n",
        "\n",
        "kdd_words = [\n",
        "    \"data mining\", \"knowledge discovery\", \"big data\", \"machine learning\", \"data analytics\", \"data preprocessing\",\n",
        "    \"clustering\", \"association rule mining\", \"pattern recognition\", \"graph mining\", \"social network analysis\",\n",
        "    \"anomaly detection\", \"classification\", \"supervised learning\", \"unsupervised learning\", \"time series analysis\",\n",
        "    \"recommendation systems\", \"neural networks\", \"ensemble methods\", \"feature extraction\", \"outlier detection\",\n",
        "    \"multi-view learning\", \"spatial data mining\", \"deep learning\", \"reinforcement learning\", \"data fusion\", \"data pipelines\",\n",
        "    \"data privacy\", \"model evaluation\", \"predictive analytics\", \"sentiment analysis\", \"market basket analysis\",\n",
        "    \"classification algorithms\", \"decision trees\", \"random forests\", \"support vector machines\", \"graph theory\",\n",
        "    \"predictive modeling\", \"predictive systems\", \"outlier detection\", \"deep reinforcement learning\", \"data visualization\",\n",
        "    \"complex networks\", \"graph theory\", \"spatial analytics\", \"distributed systems\", \"cloud computing\", \"model deployment\",\n",
        "    \"data integration\", \"data transformation\", \"data wrangling\", \"data cleaning\", \"data augmentation\", \"feature engineering\",\n",
        "    \"feature selection\", \"dimensionality reduction\", \"principal component analysis\", \"PCA\", \"t-SNE\", \"k-means\", \"k-nearest neighbors\",\n",
        "    \"SVD\", \"singular value decomposition\", \"matrix factorization\", \"latent variable models\", \"hierarchical clustering\",\n",
        "    \"DBSCAN\", \"Gaussian mixture models\", \"hidden Markov models\", \"autoencoders\", \"word embeddings\", \"semantic analysis\",\n",
        "    \"text mining\", \"text classification\", \"topic modeling\", \"latent Dirichlet allocation\", \"LDA\", \"bag of words\",\n",
        "    \"TF-IDF\", \"NLP\", \"tokenization\", \"stopword removal\", \"word2vec\", \"fastText\", \"glove\", \"sentence embeddings\", \"BERT\",\n",
        "    \"transformers\", \"transfer learning\", \"cross-validation\", \"holdout validation\", \"model selection\", \"hyperparameter tuning\",\n",
        "    \"hyperparameter optimization\", \"grid search\", \"random search\", \"Bayesian optimization\", \"cross-validation folds\",\n",
        "    \"validation sets\", \"train-test split\", \"data splitting\", \"ROC curve\", \"precision\", \"recall\", \"F1 score\", \"AUC\",\n",
        "    \"confusion matrix\", \"classification report\", \"error rate\", \"sensitivity\", \"specificity\", \"outlier analysis\", \"feature maps\",\n",
        "    \"convolutional neural networks\", \"CNN\", \"fully connected networks\", \"deep neural networks\", \"multi-layer perceptrons\",\n",
        "    \"backpropagation\", \"gradient descent\", \"stochastic gradient descent\", \"mini-batch gradient descent\", \"ADAM optimizer\",\n",
        "    \"learning rate\", \"batch normalization\", \"dropout\", \"activation functions\", \"sigmoid\", \"tanh\", \"ReLU\", \"leaky ReLU\",\n",
        "    \"softmax\", \"BCE loss\", \"categorical cross-entropy\", \"mean squared error\", \"mean absolute error\", \"loss functions\",\n",
        "    \"regularization\", \"L1 regularization\", \"L2 regularization\", \"ridge regression\", \"lasso regression\", \"elastic net\",\n",
        "    \"support vector regression\", \"kernel methods\", \"Gaussian kernels\", \"polynomial kernels\", \"RBF kernels\", \"kernel trick\",\n",
        "    \"principal component regression\", \"linear regression\", \"logistic regression\", \"ridge regression\", \"adaptive boosting\",\n",
        "    \"gradient boosting\", \"XGBoost\", \"LightGBM\", \"CatBoost\", \"AdaBoost\", \"bagging\", \"stacking\", \"voting classifiers\",\n",
        "    \"machine learning pipelines\", \"model training\", \"model testing\", \"model deployment\", \"CI/CD for ML\", \"model monitoring\",\n",
        "    \"model drift\", \"data drift\", \"model retraining\", \"real-time analytics\", \"streaming data\", \"data warehouses\",\n",
        "    \"data lakes\", \"big data processing\", \"Apache Hadoop\", \"Apache Spark\", \"MapReduce\", \"distributed computing\",\n",
        "    \"distributed learning\", \"cloud-based data storage\", \"cloud-based data processing\", \"Kubernetes\", \"Docker\", \"ML Ops\",\n",
        "    \"continuous integration\", \"continuous deployment\", \"API integration\", \"batch processing\", \"real-time processing\",\n",
        "    \"data science workflows\", \"Jupyter notebooks\", \"Google Colab\", \"machine learning APIs\", \"model endpoints\", \"data models\",\n",
        "    \"relational databases\", \"NoSQL\", \"MongoDB\", \"Cassandra\", \"HBase\", \"PostgreSQL\", \"SQL databases\", \"data retrieval\",\n",
        "    \"data cleaning algorithms\", \"data imputation\", \"missing value imputation\", \"data normalization\", \"data standardization\",\n",
        "    \"data scaling\", \"data transformations\", \"standard scaling\", \"min-max scaling\", \"robust scaling\", \"feature extraction\",\n",
        "    \"audio analysis\", \"speech recognition\", \"image recognition\", \"object detection\", \"computer vision\", \"semantic segmentation\",\n",
        "    \"instance segmentation\", \"YOLO\", \"Fast R-CNN\", \"Mask R-CNN\", \"ResNet\", \"VGG\", \"CNN architectures\", \"image processing\",\n",
        "    \"image augmentation\", \"edge detection\", \"Hough transform\", \"convolution layers\", \"pooling layers\", \"fully connected layers\",\n",
        "    \"transfer learning models\", \"pre-trained models\", \"model fine-tuning\", \"hyperparameter search\", \"ensemble learning\",\n",
        "    \"meta-learning\", \"online learning\", \"active learning\", \"semi-supervised learning\", \"unsupervised learning techniques\",\n",
        "    \"multi-class classification\", \"multi-label classification\", \"imbalanced datasets\", \"class imbalance\", \"SMOTE\",\n",
        "    \"oversampling\", \"undersampling\", \"cost-sensitive learning\", \"fairness in machine learning\", \"explainability in ML\",\n",
        "    \"model interpretability\", \"SHAP values\", \"LIME\", \"counterfactual explanations\", \"trustworthy AI\", \"bias in ML\",\n",
        "    \"algorithmic fairness\", \"privacy-preserving machine learning\", \"Federated learning\", \"differential privacy\",\n",
        "    \"secure machine learning\", \"encrypted data analysis\", \"synthetic data generation\", \"data anonymization\",\n",
        "    \"data leakage\", \"feature importance\", \"importance sampling\", \"importance-weighted classification\", \"data-driven decision making\",\n",
        "    \"risk assessment\", \"predictive maintenance\", \"fraud detection\", \"insurance modeling\", \"healthcare analytics\",\n",
        "    \"customer churn prediction\", \"market segmentation\", \"sales forecasting\", \"stock market prediction\", \"weather forecasting\",\n",
        "    \"smart cities\", \"IoT data analysis\", \"sensor data analysis\", \"biomedical data analysis\", \"genomic data analysis\",\n",
        "    \"clinical trials analysis\", \"telemedicine data\", \"sports analytics\", \"e-commerce analytics\", \"consumer behavior analysis\",\n",
        "    \"recommendation algorithms\", \"collaborative filtering\", \"content-based filtering\", \"matrix factorization\", \"latent factors\",\n",
        "    \"user-item interaction\", \"cold start problem\", \"click-through rate prediction\", \"A/B testing\", \"user experience analytics\",\n",
        "    \"sentiment classification\", \"emotion detection\", \"text classification\", \"document clustering\", \"entity recognition\",\n",
        "    \"named entity recognition\", \"topic clustering\", \"web mining\", \"social media mining\", \"web scraping\", \"data crawling\",\n",
        "    \"data annotation\", \"data labeling\", \"labeling tools\", \"crowdsourcing for data labeling\", \"data validation\", \"data governance\",\n",
        "    \"data ethics\", \"GDPR compliance\", \"data stewardship\", \"data provenance\", \"data lineage\", \"data quality\", \"data accuracy\",\n",
        "    \"data consistency\", \"data completeness\", \"data validity\", \"data redundancy\", \"data consistency checks\", \"data traceability\",\n",
        "    \"data warehousing\", \"ETL processes\", \"data pipeline automation\", \"batch data processing\", \"data synchronization\",\n",
        "    \"event-driven data processing\", \"data shuffling\", \"feature pipeline\", \"data pipeline architecture\", \"data processing frameworks\",\n",
        "    \"IoT analytics\", \"real-time dashboards\", \"business intelligence\", \"executive analytics\", \"enterprise data analysis\",\n",
        "    \"smart analytics\", \"data democratization\", \"AI-driven data science\", \"data visualization tools\", \"Power BI\", \"Tableau\",\n",
        "    \"Matplotlib\", \"Seaborn\", \"Plotly\", \"D3.js\", \"data storytelling\", \"data-driven insights\", \"interactive visualizations\",\n",
        "    \"geospatial data analysis\", \"location-based services\", \"GPS data analysis\", \"map-based data visualization\", \"geospatial prediction\",\n",
        "    \"geo-fencing\", \"spatial data models\", \"spatio-temporal data\", \"geospatial clustering\", \"geospatial pattern recognition\",\n",
        "    \"geo-statistics\", \"multivariate analysis\", \"cognitive computing\", \"data-centric AI\", \"data pipeline management\",\n",
        "    \"data system monitoring\", \"elastic search\", \"big data analytics\", \"real-time data stream processing\", \"data warehouse architecture\",\n",
        "    \"data lakehouse\", \"data mesh architecture\", \"databases as a service\", \"cloud data solutions\", \"hybrid cloud solutions\",\n",
        "    \"data orchestration\", \"distributed analytics\", \"data mining applications\", \"big data challenges\", \"data science operations\"\n",
        "]"
      ],
      "metadata": {
        "id": "J95Dupb5wtzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conference_keywords={\"CVPR\":cvpr_words,\"NUERIPS\":neurips_words,\"EMNLP\":emnlp_words,\"TMLR\":tmlr_words,\"KDD\":kdd_words}"
      ],
      "metadata": {
        "id": "mg7kBnqfuGJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Compute embeddings for each word in conference keywords\n",
        "def compute_word_embeddings(keywords, model, tokenizer):\n",
        "    \"\"\"\n",
        "    Compute embeddings for each word in the conference keywords using SciBERT.\n",
        "\n",
        "    Args:\n",
        "        keywords (list): List of keywords for the conference.\n",
        "        model: SciBERT model.\n",
        "        tokenizer: SciBERT tokenizer.\n",
        "\n",
        "    Returns:\n",
        "        list: List of word embeddings for the keywords.\n",
        "    \"\"\"\n",
        "    embeddings = []\n",
        "    for keyword in keywords:\n",
        "        inputs = tokenizer(keyword, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "        embedding = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
        "        embeddings.append(embedding)\n",
        "    return embeddings\n",
        "\n",
        "\n",
        "# Compute word embeddings for each conference\n",
        "conference_word_embeddings = {\n",
        "    conference: compute_word_embeddings(keywords, model, tokenizer)\n",
        "    for conference, keywords in conference_keywords.items()\n",
        "}\n",
        "\n",
        "# Step 2: Compute average similarity for a paper with each conference\n",
        "def compute_average_similarity(paper_embedding, conference_word_embeddings):\n",
        "    \"\"\"\n",
        "    Compute the average cosine similarity between a paper's embedding and a conference's word embeddings.\n",
        "\n",
        "    Args:\n",
        "        paper_embedding (numpy.array): Embedding of the research paper.\n",
        "        conference_word_embeddings (list): List of word embeddings for a conference.\n",
        "\n",
        "    Returns:\n",
        "        float: The average cosine similarity.\n",
        "    \"\"\"\n",
        "    similarities = []\n",
        "    for word_embedding in conference_word_embeddings:\n",
        "        similarity = cosine_similarity([paper_embedding], [word_embedding])[0, 0]\n",
        "        similarities.append(similarity)\n",
        "    return np.mean(similarities) if similarities else 0\n",
        "\n",
        "# Step 3: Assign conference to each paper\n",
        "def assign_conference_by_similarity(paper_embedding, conference_word_embeddings):\n",
        "    \"\"\"\n",
        "    Assign the most suitable conference to a paper based on average similarity.\n",
        "\n",
        "    Args:\n",
        "        paper_embedding (numpy.array): Embedding of the research paper.\n",
        "        conference_word_embeddings (dict): Dictionary of word embeddings for all conferences.\n",
        "\n",
        "    Returns:\n",
        "        str: The name of the most similar conference.\n",
        "    \"\"\"\n",
        "    best_conference = None\n",
        "    max_similarity = -1\n",
        "\n",
        "    for conference, word_embeddings in conference_word_embeddings.items():\n",
        "        avg_similarity = compute_average_similarity(paper_embedding, word_embeddings)\n",
        "        if avg_similarity > max_similarity:\n",
        "            max_similarity = avg_similarity\n",
        "            best_conference = conference\n",
        "\n",
        "    return best_conference\n",
        "\n",
        "# Step 4: Predict conferences for all papers\n",
        "df['predicted_conference'] = [\n",
        "    assign_conference_by_similarity(embedding, conference_word_embeddings)\n",
        "    for embedding in embeddings\n",
        "]\n",
        "\n",
        "# Output predictions\n",
        "print(df[['texty', 'predicted_conference']])\n"
      ],
      "metadata": {
        "id": "1LlffieLuM1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['predicted_conference'].value_counts()"
      ],
      "metadata": {
        "id": "XYHUI_08uQEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.spatial.distance import euclidean\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Precompute average embeddings for each conference\n",
        "def compute_average_embedding(keywords, model, tokenizer):\n",
        "    \"\"\"\n",
        "    Compute the average embedding for a list of keywords using SciBERT.\n",
        "\n",
        "    Args:\n",
        "        keywords (list): List of keywords for a conference.\n",
        "        model: SciBERT model.\n",
        "        tokenizer: SciBERT tokenizer.\n",
        "\n",
        "    Returns:\n",
        "        numpy.array: Average embedding of the keywords.\n",
        "    \"\"\"\n",
        "    embeddings = []\n",
        "    for keyword in keywords:\n",
        "        inputs = tokenizer(keyword, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "        embedding = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
        "        embeddings.append(embedding)\n",
        "    return np.mean(embeddings, axis=0) if embeddings else np.zeros(model.config.hidden_size)\n",
        "\n",
        "# Precompute average embeddings for all conferences\n",
        "conference_avg_embeddings = {\n",
        "    conference: compute_average_embedding(keywords, model, tokenizer)\n",
        "    for conference, keywords in conference_keywords.items()\n",
        "}\n",
        "\n",
        "# Step 2: Compute Euclidean distance and assign conference\n",
        "def assign_conference_by_euclidean(paper_embedding, conference_avg_embeddings):\n",
        "    \"\"\"\n",
        "    Assign the closest conference to a paper based on Euclidean distance.\n",
        "\n",
        "    Args:\n",
        "        paper_embedding (numpy.array): Embedding of the research paper.\n",
        "        conference_avg_embeddings (dict): Dictionary of average embeddings for all conferences.\n",
        "\n",
        "    Returns:\n",
        "        str: The name of the closest conference.\n",
        "    \"\"\"\n",
        "    best_conference = None\n",
        "    min_distance = float('inf')\n",
        "\n",
        "    for conference, avg_embedding in conference_avg_embeddings.items():\n",
        "        distance = euclidean(paper_embedding, avg_embedding)\n",
        "        if distance < min_distance:\n",
        "            min_distance = distance\n",
        "            best_conference = conference\n",
        "\n",
        "    return best_conference\n",
        "\n",
        "# Step 3: Predict conferences for all papers\n",
        "df['predicted_conference_2'] = [\n",
        "    assign_conference_by_euclidean(embedding, conference_avg_embeddings)\n",
        "    for embedding in embeddings\n",
        "]\n",
        "\n",
        "# Output predictions\n",
        "print(df[['texty', 'predicted_conference_2']])\n"
      ],
      "metadata": {
        "id": "sd4CfvdcuSN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conference_keywords_2 = {\n",
        "    \"CVPR\": [\n",
        "        \"computer vision\", \"image processing\", \"object detection\", \"image segmentation\", \"3D reconstruction\",\n",
        "        \"pose estimation\", \"scene understanding\", \"visual recognition\", \"optical flow\", \"action recognition\",\n",
        "        \"video analysis\", \"visual tracking\", \"image classification\", \"stereo vision\", \"depth estimation\",\n",
        "        \"feature extraction\", \"visual data\", \"image restoration\", \"camera calibration\", \"visual feature matching\",\n",
        "        \"image synthesis\", \"neural networks for vision\", \"convolutional neural networks\", \"generative adversarial networks\",\n",
        "        \"scene analysis\", \"motion analysis\", \"visual perception\", \"spatial transformations\"\n",
        "    ],\n",
        "    \"EMNLP\": [\n",
        "        \"natural language processing\", \"text mining\", \"machine translation\", \"text summarization\", \"question answering\",\n",
        "        \"dialogue systems\", \"semantic parsing\", \"named entity recognition\", \"text generation\", \"sentiment analysis\",\n",
        "        \"language modeling\", \"text classification\", \"dependency parsing\", \"transformers\", \"word embeddings\", \"language understanding\",\n",
        "        \"syntax parsing\", \"discourse analysis\", \"textual data\", \"tokenization\", \"speech recognition\",\n",
        "        \"morphological analysis\", \"semantic analysis\", \"cross-lingual tasks\", \"language resources\",\n",
        "        \"question generation\", \"coreference resolution\", \"textual entailment\", \"neural machine translation\"\n",
        "    ],\n",
        "    \"NEURIPS\": [\n",
        "        \"machine learning\", \"reinforcement learning\", \"deep learning\", \"neural networks\", \"variational inference\",\n",
        "        \"self-supervised learning\", \"meta-learning\", \"federated learning\", \"generative models\", \"optimization\",\n",
        "        \"Bayesian inference\", \"graph neural networks\", \"unsupervised learning\", \"supervised learning\", \"neural architecture\",\n",
        "        \"model training\", \"probabilistic models\", \"decision making\", \"active learning\", \"clustering\",\n",
        "        \"evolutionary algorithms\", \"data-driven models\", \"causal inference\", \"statistical models\", \"optimization algorithms\",\n",
        "        \"transformer models\", \"computational neuroscience\", \"deep reinforcement learning\", \"algorithmic fairness\"\n",
        "    ],\n",
        "    \"TMLR\": [\n",
        "        \"machine learning theory\", \"learning theory\", \"theoretical ML\", \"generalization bounds\", \"PAC learning\",\n",
        "        \"non-convex optimization\", \"convex optimization\", \"kernel methods\", \"spectral methods\", \"dimensionality reduction\",\n",
        "        \"statistical learning\", \"algorithmic fairness\", \"information theory\", \"mathematical foundations\", \"optimization theory\",\n",
        "        \"function approximation\", \"bias-variance tradeoff\", \"learning algorithms\", \"statistical models\", \"generalization error\",\n",
        "        \"theoretical analysis\", \"robust learning\", \"stochastic processes\", \"mathematical optimization\", \"empirical risk minimization\"\n",
        "    ],\n",
        "    \"KDD\": [\n",
        "        \"data mining\", \"data analysis\", \"big data\", \"knowledge discovery\", \"data-driven decision making\",\n",
        "        \"anomaly detection\", \"data visualization\", \"feature engineering\", \"scalable algorithms\", \"graph mining\",\n",
        "        \"spatiotemporal analysis\", \"recommendation systems\", \"predictive modeling\", \"real-time data processing\",\n",
        "        \"unsupervised learning\", \"classification\", \"data preprocessing\", \"clustering\", \"association rule learning\",\n",
        "        \"data pipelines\", \"pattern recognition\", \"data integration\", \"data exploration\", \"forecasting\",\n",
        "        \"decision support systems\", \"data retrieval\", \"large-scale data\", \"data quality\", \"behavior analysis\"\n",
        "    ]\n",
        "}\n"
      ],
      "metadata": {
        "id": "zYTtIHS0uVhr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Compute embeddings for each word in conference keywords\n",
        "def compute_word_embeddings(keywords, model, tokenizer):\n",
        "    \"\"\"\n",
        "    Compute embeddings for each word in the conference keywords using SciBERT.\n",
        "\n",
        "    Args:\n",
        "        keywords (list): List of keywords for the conference.\n",
        "        model: SciBERT model.\n",
        "        tokenizer: SciBERT tokenizer.\n",
        "\n",
        "    Returns:\n",
        "        list: List of word embeddings for the keywords.\n",
        "    \"\"\"\n",
        "    embeddings = []\n",
        "    for keyword in keywords:\n",
        "        inputs = tokenizer(keyword, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "        embedding = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
        "        embeddings.append(embedding)\n",
        "    return embeddings\n",
        "\n",
        "\n",
        "# Compute word embeddings for each conference\n",
        "conference_word_embeddings = {\n",
        "    conference: compute_word_embeddings(keywords, model, tokenizer)\n",
        "    for conference, keywords in conference_keywords_2.items()\n",
        "}\n",
        "\n",
        "# Step 2: Compute average similarity for a paper with each conference\n",
        "def compute_average_similarity(paper_embedding, conference_word_embeddings):\n",
        "    \"\"\"\n",
        "    Compute the average cosine similarity between a paper's embedding and a conference's word embeddings.\n",
        "\n",
        "    Args:\n",
        "        paper_embedding (numpy.array): Embedding of the research paper.\n",
        "        conference_word_embeddings (list): List of word embeddings for a conference.\n",
        "\n",
        "    Returns:\n",
        "        float: The average cosine similarity.\n",
        "    \"\"\"\n",
        "    similarities = []\n",
        "    for word_embedding in conference_word_embeddings:\n",
        "        similarity = cosine_similarity([paper_embedding], [word_embedding])[0, 0]\n",
        "        similarities.append(similarity)\n",
        "    return np.mean(similarities) if similarities else 0\n",
        "\n",
        "# Step 3: Assign conference to each paper\n",
        "def assign_conference_by_similarity(paper_embedding, conference_word_embeddings):\n",
        "    \"\"\"\n",
        "    Assign the most suitable conference to a paper based on average similarity.\n",
        "\n",
        "    Args:\n",
        "        paper_embedding (numpy.array): Embedding of the research paper.\n",
        "        conference_word_embeddings (dict): Dictionary of word embeddings for all conferences.\n",
        "\n",
        "    Returns:\n",
        "        str: The name of the most similar conference.\n",
        "    \"\"\"\n",
        "    best_conference = None\n",
        "    max_similarity = -1\n",
        "\n",
        "    for conference, word_embeddings in conference_word_embeddings.items():\n",
        "        avg_similarity = compute_average_similarity(paper_embedding, word_embeddings)\n",
        "        if avg_similarity > max_similarity:\n",
        "            max_similarity = avg_similarity\n",
        "            best_conference = conference\n",
        "\n",
        "    return best_conference\n",
        "\n",
        "# Step 4: Predict conferences for all papers\n",
        "df['predicted_conference_3'] = [\n",
        "    assign_conference_by_similarity(embedding, conference_word_embeddings)\n",
        "    for embedding in embeddings\n",
        "]\n",
        "\n",
        "# Output predictions\n",
        "print(df[['texty', 'predicted_conference_3']])\n"
      ],
      "metadata": {
        "id": "hgofh2JZucOB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['predicted_conference_3'].value_counts()"
      ],
      "metadata": {
        "id": "hTRR-dVlugka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# install dependencies\n",
        "!pip install transformers\n",
        "!pip install einops\n",
        "!pip install accelerate"
      ],
      "metadata": {
        "id": "epYGhoLsuhIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece\n"
      ],
      "metadata": {
        "id": "1WimQvsVujLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch                        # allows Tensor computation with strong GPU acceleration\n",
        "from transformers import pipeline   # fast way to use pre-trained models for inference\n",
        "import os"
      ],
      "metadata": {
        "id": "2udf-oicuk4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, pipeline\n",
        "# Load pre-trained T5 model and tokenizer\n",
        "model_name = 't5-large'  # You can also use 't5-base', 't5-large' based on your needs\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n"
      ],
      "metadata": {
        "id": "Tyw0n_JYuns0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Abst\"]=np.nan\n",
        "for i in range(150):\n",
        "  df[\"Abst\"][i]=df['text_dict'][i][\"Abstract\"]"
      ],
      "metadata": {
        "id": "2MfdPT9ZgN2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Initialize the T5 model and tokenizer for text-to-text generation\n",
        "question_answerer = pipeline(\"text2text-generation\", model=\"t5-small\", tokenizer=\"t5-small\")\n",
        "\n",
        "# Define a function to generate conference justification based on the paper's abstract\n",
        "def generate_conference_classification(paper_abstract):\n",
        "    # List of conferences (this can be expanded or modified as needed)\n",
        "    conferences = [\"kdd\", \"neurips\", \"emnlp\", \"cvpr\", \"tmlr\"]\n",
        "\n",
        "    # Generate a question about which conference the paper belongs to\n",
        "    question = f\"Which conference does this paper belong to among {', '.join(conferences)}?\"\n",
        "\n",
        "    kk=paper_abstract+textt\n",
        "\n",
        "    textt=\"KDD focuses on data mining, NeurIPS on neural networks and AI, EMNLP on natural language processing, CVPR on computer vision, and TMLR on reproducible machine learning research\"\n",
        "\n",
        "    # Combine the question with the context (paper content)\n",
        "    input_text = f\"question: {question} context: {kk}\"\n",
        "\n",
        "    # Check if the input text exceeds the model's token limit (512 tokens for T5-small)\n",
        "    tokenizer = question_answerer.tokenizer\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
        "\n",
        "    # Generate the answer with the model\n",
        "    answer = question_answerer(input_text, max_length=512, do_sample=False)\n",
        "\n",
        "    # Return the generated justification (answer)\n",
        "    return answer[0]['generated_text']\n",
        "\n"
      ],
      "metadata": {
        "id": "gQrvp-gAuqJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['conf_4']=np.nan\n",
        "for i in range(150):\n",
        "  df['conf_4'][i]=generate_conference_classification(df[\"Abst\"][i])\n"
      ],
      "metadata": {
        "id": "ApbT9h0_gfND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prioritize_predictions(pred1, pred2, pred3, pred4, pred5):\n",
        "    \"\"\"\n",
        "    Prioritize predictions based on a fair balance of weights and consensus:\n",
        "    1. pred1 has the highest priority but can be overridden if a strong consensus exists.\n",
        "    2. Check for consensus among pred2, pred3, pred4, and pred5.\n",
        "    3. If no strong consensus, prioritize pred1.\n",
        "    4. Use a fallback value like \"Undecided\" if all predictions are None.\n",
        "    \"\"\"\n",
        "    from collections import Counter\n",
        "\n",
        "    # Count the occurrences of each prediction (ignoring None)\n",
        "    predictions = [pred2, pred3, pred4, pred5]\n",
        "    prediction_counts = Counter(p for p in predictions if p is not None)\n",
        "\n",
        "    # Determine the most common prediction and its frequency\n",
        "    most_common, frequency = prediction_counts.most_common(1)[0] if prediction_counts else (None, 0)\n",
        "\n",
        "    # Rule 1: If pred1 is not None and aligns with consensus, prioritize pred1\n",
        "    if pred1 is not None and pred1 == most_common:\n",
        "        return pred1\n",
        "\n",
        "    # Rule 2: If a strong consensus exists (at least 3 out of 4 agree), use it\n",
        "    if frequency >= 3:\n",
        "        return most_common\n",
        "\n",
        "    # Rule 3: If no strong consensus, prioritize pred1 if not None\n",
        "    if pred1 is not None:\n",
        "        return pred1\n",
        "\n",
        "    # Rule 4: Default case (no consensus, pred1 is None)\n",
        "    return None  # or a fallback value like \"Undecided\"\n"
      ],
      "metadata": {
        "id": "MfM3We7D1S3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['confp']=df.apply(lambda row: prioritize_predictions(row['conf'],row['predicted_conference'], row['predicted_conference_2'], row['predicted_conference_3'], row['conf_4']), axis=1)"
      ],
      "metadata": {
        "id": "zJAe6jZph9Oo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Abst\"]"
      ],
      "metadata": {
        "id": "4IGUekIbu9rJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import pandas as pd\n",
        "\n",
        "# Step 1: Initialize the summarizer pipeline\n",
        "summarizer = pipeline(\"summarization\", model=\"t5-large\", tokenizer=\"t5-large\")\n",
        "\n",
        "# Step 2: Generate a paper summary using T5 model\n",
        "def generate_summary(text):\n",
        "    # Use the summarizer pipeline to generate the summary\n",
        "    summary = summarizer(text, max_length=100, min_length=50, do_sample=False)\n",
        "    return summary[0]['summary_text']\n",
        "conference_mapping = {\n",
        "    \"kdd\": \"Knowledge Discovery and Data Mining\",\n",
        "    \"neurips\": \"Conference on Neural Information Processing Systems\",\n",
        "    \"cvpr\": \"Conference on Computer Vision and Pattern Recognition\",\n",
        "    \"tmlr\": \"Transactions on Machine Learning Research\",\n",
        "    \"emnlp\": \"Conference on Empirical Methods in Natural Language Processing\"\n",
        "}\n",
        "\n",
        "# Step 3: Combine provided keywords and summary for justification generation\n",
        "def generate_justification(paper_text, topic_keywords, conference_abbr):\n",
        "    # Select top 10 keywords (use all of them if less than 10)\n",
        "    selected_keywords = topic_keywords[:10]\n",
        "    conference_full_name = conference_mapping.get(conference_abbr, \"Unknown Conference\")\n",
        "\n",
        "\n",
        "    # Generate paper summary\n",
        "    summary = generate_summary(paper_text)\n",
        "\n",
        "    # Generate justification in the desired format\n",
        "    justification = f\"It is classified into {conference_abbr.upper()} ({conference_full_name}) because the work on {summary}, combined with the key themes of {', '.join(selected_keywords)}, aligns well with the themes of {conference_abbr.upper()}. {conference_abbr.upper()} focuses on advancements in {conference_full_name} research, making it an appropriate conference for your research.\"\n",
        "\n",
        "    return justification\n"
      ],
      "metadata": {
        "id": "fJG71XgGvAFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['just']=np.nan"
      ],
      "metadata": {
        "id": "eEScCd4LvCwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(df)):\n",
        "    if(df[\"confp\"][i]!=\"na\"):\n",
        "    # Get the conference abbreviation for this row (already in lowercase)\n",
        "    conference_abbr = df[\"conference\"][i]\n",
        "\n",
        "    # Generate justification for the paper\n",
        "    justification = generate_justification(df[\"Abst\"][i], df[\"topic_keywords\"][i], conference_abbr)\n",
        "\n",
        "    # Store the justification in the new column 'just'\n",
        "    df.at[i, 'just'] = justification\n",
        "    else:\n",
        "      df.at[i, 'just'] = \"na\""
      ],
      "metadata": {
        "id": "qx03skThwNdR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter the DataFrame starting from index 15\n",
        "df_filtered = df.loc[15:, ['y_fin', 'confp', 'just']]\n",
        "\n",
        "# Save the filtered DataFrame to a CSV file\n",
        "df_filtered.to_csv('output_f.csv', index=False)\n",
        "\n",
        "print(\"CSV file has been saved as 'output_filtered.csv'\")\n"
      ],
      "metadata": {
        "id": "JNk1QCYUlP6L"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
